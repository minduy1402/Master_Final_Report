%% exemplo de livro
@book{kn:Bel02-book,
  title     = {Como realizar um projeto de investigação},
  author    = {Bell, Judith},
  publisher = {Gradiva. Lisboa},
  year      = {2002}
}

%% exemplo de relatório técnico
@techreport{kn:VKL+18-dtu,
  title       = {{e-WindLidar: making wind lidar data FAIR}},
  author      = {Vasiljevic, Nikola and Lopes, J. Correia and Gomes, Daniel},
  institution = {DTU, Denmark},
  doi         = {10.5281/zenodo.2478051},
  year        = {2018}
}

%% exemplo de artigo científico
@article{kn:GLPR14-joPhysics,
  title     = {{WindS@UP: The e-Science Platform for WindScanner.eu}},
  author    = {Gomes, Filipe and Lopes, J. Correia and Palma, Jos{\'{e}} Laginha},
  doi       = {10.1088/1742-6596/524/1/012006},
  issn      = {17426596},
  journal   = {Journal of Physics: Conference Series},
  number    = {The Science of Making Torque from Wind 2014 (TORQUE 2014) 18–20 June 2014, Copenhagen, Denmark},
  publisher = {Poster presented at Torque2014: The Science of Making Torque From Wind, 2014 --- 17-20 June 2014 Technical University of Denmark, Lyngby, Copenhagen},
  volume    = {524},
  pages     = {012006},
  year      = {2014}
}

%% exemplo de comunicação em conferência
@inproceedings{kn:MSS+12-wemep,
  title     = {{WindScanner.eu --- a new Remote Sensing Research Infrastructure for On- and Offshore Wind Energy}},
  author    = {Mikkelsen, Torben},
  booktitle = {Proceedings of the International Conference on Wind Energy: Materials, Engineering, and Policies (WEMEP-2012)},
  url       = {www.windscanner.eu},
  year      = {2012}
}

%% exemplo de artigo científico
@inproceedings{kn:GLPC22-torque,
  title     = {{\windspt{} e-Science platform for wind measurement campaigns}},
  author    = {Gomes, D. and Lopes, J. Correia and Palma},
  booktitle = {TORQUE2022 (EAW's Science of Making Torque from Wind)},
  year      = {2022}
}

%% exemplo de fonte da imagem
@misc{kn:figura,
  title        = {{Faculdade de Engenharia celebra 20 anos no Polo da Asprela}},
  author       = {{Espaço de Arquitetura}},
  howpublished = {Disponível em \url{https://espacodearquitetura.com/noticias/faculdade-de-engenharia-celebra-20-anos-no-polo-da-asprela/}},
  year         = {2020}
}

%% exemplo de fonte dos dados da tabela
@misc{kn:tabela,
  title        = {{FEUP} em Números 2023},
  author       = {FEUP},
  howpublished = {Disponível em \url{https://sigarra.up.pt/feup/pt/web_base.gera_pagina?p_pagina=258031}},
  year         = {2023}
}

%% example of MsC dissertation
@mastersthesis{ornelas2016,
  title  = {Platform for monitoring and treat depression},
  author = {Jos{\'e} Pedro Alves Ornelas},
  school = {Universidade do Porto (Portugal)},
  year   = {2016}
}

%% outro
@misc{kn:Lip08,
  title   = {Lorem Ipsum},
  author  = {Lipsum},
  url     = {http://www.lipsum.com/},
  urldate = {2023-11-15},
  year    = {2023}
}

@misc{gantt,
  author  = {wikipedia},
  title   = {Gantt chart},
  url     = {https://en.wikipedia.org/wiki/Gantt\_chart},
  urldate = {2023-03-15},
  year    = {2023}
}

@article{despa2014comparative,
  title   = {Comparative study on software development methodologies},
  author  = {Despa, Mihai Liviu},
  journal = {Database Systems Journal},
  volume  = {5},
  number  = {3},
  pages   = {37--56},
  year    = {2014}
}

@article{knuth:1984,
  title     = {Literate Programming},
  author    = {Donald E. Knuth},
  journal   = {The Computer Journal},
  volume    = {27},
  number    = {2},
  pages     = {97--111},
  year      = {1984},
  publisher = {Oxford University Press}
}

@inproceedings{lesk:1977,
  title     = {Computer Typesetting of Technical Journals on {UNIX}},
  author    = {Michael Lesk and Brian Kernighan},
  booktitle = {Proceedings of American Federation of Information Processing
               Societies: 1977 National Computer Conference},
  pages     = {879--888},
  year      = {1977},
  address   = {Dallas, Texas}
}

@techreport{iso_19156_2011,
  title       = {Geographic information -- Observations and measurements},
  author      = {ISO},
  institution = {International Organization for Standardization},
  year        = {2011},
  type        = {standard},
  number      = {ISO 19156:2011}
}


%  ============================================================================
%% Bibliography for Master's Thesis
%% Topic: Optimizing and Adapting Language Models for Domain-Specific Tasks

%% ============================================
%% FOUNDATIONAL PAPERS
%% ============================================

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

%% ============================================
%% LARGE LANGUAGE MODELS
%% ============================================

@article{achiam2023gpt,
  title   = {Gpt-4 technical report},
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@misc{anthropic2024claude,
  author       = {{Anthropic}},
  title        = {Claude 3 Model Card},
  year         = {2024},
  howpublished = {\url{https://www.anthropic.com/claude}},
  note         = {Accessed: 2025}
}

@article{comanici2025gemini,
  title   = {Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author  = {Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal = {arXiv preprint arXiv:2507.06261},
  year    = {2025}
}

@article{touvron2023llama,
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}
@article{grattafiori2024llama,
  title   = {The llama 3 herd of models},
  author  = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal = {arXiv preprint arXiv:2407.21783},
  year    = {2024}
}

@article{team2024qwen2,
  title   = {Qwen2 technical report},
  author  = {Team, Qwen and others},
  journal = {arXiv preprint arXiv:2412.15115v2},
  volume  = {2},
  number  = {3},
  year    = {2024}
}
@article{team2024qwen2.5,
  author     = {An Yang and
                Baosong Yang and
                Beichen Zhang and
                Binyuan Hui and
                Bo Zheng and
                Bowen Yu and
                Chengyuan Li and
                Dayiheng Liu and
                Fei Huang and
                Haoran Wei and
                Huan Lin and
                Jian Yang and
                Jianhong Tu and
                Jianwei Zhang and
                Jianxin Yang and
                Jiaxi Yang and
                Jingren Zhou and
                Junyang Lin and
                Kai Dang and
                Keming Lu and
                Keqin Bao and
                Kexin Yang and
                Le Yu and
                Mei Li and
                Mingfeng Xue and
                Pei Zhang and
                Qin Zhu and
                Rui Men and
                Runji Lin and
                Tianhao Li and
                Tingyu Xia and
                Xingzhang Ren and
                Xuancheng Ren and
                Yang Fan and
                Yang Su and
                Yichang Zhang and
                Yu Wan and
                Yuqiong Liu and
                Zeyu Cui and
                Zhenru Zhang and
                Zihan Qiu},
  title      = {Qwen2.5 Technical Report},
  journal    = {CoRR},
  volume     = {abs/2412.15115},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2412.15115},
  doi        = {10.48550/ARXIV.2412.15115},
  eprinttype = {arXiv},
  eprint     = {2412.15115},
  timestamp  = {Sat, 15 Nov 2025 16:38:56 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2412-15115.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

%% ============================================
%% RAG AND KNOWLEDGE AUGMENTATION
%% ============================================

@inproceedings{lewis2020rag,
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  title     = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {9459--9474},
  year      = {2020}
}
@article{asai2024selfrag,
  author  = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avi and Hajishirzi, Hannaneh},
  title   = {Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  journal = {arXiv preprint arXiv:2310.11511},
  year    = {2024}
}

%% ============================================
%% HALLUCINATION
%% ============================================

@article{ji2023hallucination,
  author    = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  title     = {Survey of Hallucination in Natural Language Generation},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {12},
  pages     = {1--38},
  year      = {2023},
  publisher = {ACM}
}

@article{huang2023hallucination_survey,
  author  = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  title   = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  journal = {arXiv preprint arXiv:2311.05232},
  year    = {2023}
}

%% ============================================
%% FINE-TUNING AND PEFT
%% ============================================

@inproceedings{hu2022lora,
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{dettmers2024qlora,
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title   = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{ding2023peft_survey,
  author    = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  title     = {Parameter-Efficient Fine-Tuning of Large-Scale Pre-Trained Language Models},
  journal   = {Nature Machine Intelligence},
  volume    = {5},
  number    = {3},
  pages     = {220--235},
  year      = {2023},
  publisher = {Nature Publishing Group}
}

%% ============================================
%% SCALING LAWS AND EFFICIENCY
%% ============================================

@inproceedings{hoffmann2022chinchilla,
  author    = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  title     = {Training Compute-Optimal Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {30016--30030},
  year      = {2022}
}

@article{kaplan2020scaling,
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title   = {Scaling Laws for Neural Language Models},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}

%% ============================================
%% EVALUATION AND BENCHMARKS
%% ============================================

@article{zheng2023llmjudge,
  author  = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and others},
  title   = {Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2023}
}

@inproceedings{es2024ragas,
  author    = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  title     = {{RAGAS}: Automated Evaluation of Retrieval Augmented Generation},
  booktitle = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
  pages     = {150--163},
  year      = {2024}
}

%% ============================================
%% SYNTHETIC DATA AND KNOWLEDGE DISTILLATION
%% ============================================

@article{taori2023alpaca,
  author  = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  title   = {Stanford Alpaca: An Instruction-following {LLaMA} Model},
  journal = {GitHub repository},
  year    = {2023},
  url     = {https://github.com/tatsu-lab/stanford_alpaca}
}

@article{wang2023selfinstruct,
  author  = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  title   = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  journal = {arXiv preprint arXiv:2212.10560},
  year    = {2023}
}

@article{xu2024survey_distillation,
  author  = {Xu, Canwen and McAuley, Julian},
  title   = {A Survey on Knowledge Distillation of Large Language Models},
  journal = {arXiv preprint arXiv:2402.13116},
  year    = {2024}
}

%% ============================================
%% DOCUMENT UNDERSTANDING AND MULTIMODAL
%% ============================================

@inproceedings{huang2022layoutlmv3,
  author    = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  title     = {{LayoutLMv3}: Pre-training for Document {AI} with Unified Text and Image Masking},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages     = {4083--4091},
  year      = {2022}
}

@article{li2024multimodal_rag,
  author  = {Li, Jiayuan and Sun, Tianyu and Zhang, Zhiyuan},
  title   = {A Survey on Multimodal Retrieval-Augmented Generation},
  journal = {arXiv preprint arXiv:2402.15052},
  year    = {2024}
}

%% ============================================
%% EMBEDDINGS AND RETRIEVAL
%% ============================================

@article{reimers2019sentencebert,
  author  = {Reimers, Nils and Gurevych, Iryna},
  title   = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
  journal = {arXiv preprint arXiv:1908.10084},
  year    = {2019}
}

@article{karpukhin2020dpr,
  author  = {Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  title   = {Dense Passage Retrieval for Open-Domain Question Answering},
  journal = {arXiv preprint arXiv:2004.04906},
  year    = {2020}
}

@article{robertson2009bm25,
  author  = {Robertson, Stephen and Zaragoza, Hugo},
  title   = {The Probabilistic Relevance Framework: {BM25} and Beyond},
  journal = {Foundations and Trends in Information Retrieval},
  volume  = {3},
  number  = {4},
  pages   = {333--389},
  year    = {2009}
}

@article{robertson2004understanding,
  title     = {Understanding inverse document frequency: on theoretical arguments for IDF},
  author    = {Robertson, Stephen},
  journal   = {Journal of documentation},
  volume    = {60},
  number    = {5},
  pages     = {503--520},
  year      = {2004},
  publisher = {Emerald Group Publishing Limited}
}

@inproceedings{cormack2009reciprocal,
  title     = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  author    = {Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},
  booktitle = {Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages     = {758--759},
  year      = {2009}
}

%% ============================================
%% VECTOR DATABASES
%% ============================================

@article{johnson2019faiss,
  author    = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  title     = {Billion-scale similarity search with {GPUs}},
  journal   = {IEEE Transactions on Big Data},
  volume    = {7},
  number    = {3},
  pages     = {535--547},
  year      = {2019},
  publisher = {IEEE}
}

%% ============================================
%% INDUSTRY REPORTS
%% ============================================

@techreport{mckinsey2024genai,
  author      = {{McKinsey Global Institute}},
  title       = {The Economic Potential of Generative {AI}: The Next Productivity Frontier},
  institution = {McKinsey \& Company},
  year        = {2024},
  url         = {https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier}
}

@techreport{gartner2024genai,
  author      = {{Gartner}},
  title       = {Generative {AI} Adoption Survey: Barriers and Best Practices},
  institution = {Gartner, Inc.},
  year        = {2024},
  url         = {https://www.gartner.com/en/topics/generative-ai}
}

%% ============================================
%% FRAMEWORKS AND TOOLS
%% ============================================

@misc{langchain2023,
  author       = {Chase, Harrison},
  title        = {LangChain: Building Applications with {LLMs} through Composability},
  year         = {2023},
  howpublished = {\url{https://github.com/langchain-ai/langchain}},
  note         = {GitHub repository}
}

@misc{llamaindex2023,
  author       = {Liu, Jerry and others},
  title        = {{LlamaIndex}: Data Framework for {LLM} Applications},
  year         = {2023},
  howpublished = {\url{https://github.com/run-llama/llama_index}},
  note         = {GitHub repository}
}

%% ============================================
%% REINFORCEMENT LEARNING FROM HUMAN FEEDBACK
%% ============================================

@article{ouyang2022instructgpt,
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  title   = {Training Language Models to Follow Instructions with Human Feedback},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {27730--27744},
  year    = {2022}
}

@article{rafailov2024dpo,
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D. and Ermon, Stefano and Finn, Chelsea},
  title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

%% ============================================
%% SMALL LANGUAGE MODELS
%% ============================================

@article{abdin2024phi3,
  author  = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Beber, Harkirat and others},
  title   = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  journal = {arXiv preprint arXiv:2404.14219},
  year    = {2024}
}

@article{gunasekar2023textbooks,
  author  = {Gunasekar, Suriya and Zhang, Yi and Anber, Jyoti and Awadallah, Ahmed and Bajaj, Payal and Chaudhary, Vishrav and Garza, Antonio and Gopal, Alon and Jaiswal, Parul and Kauffmann, Johannes and others},
  title   = {Textbooks Are All You Need},
  journal = {arXiv preprint arXiv:2306.11644},
  year    = {2023}
}

%% ============================================
%% ADD YOUR REFERENCES HERE
%% ============================================

@online{tomshardware2024mit,
  author       = {Freedman, Andrew},
  title        = {95 Percent of Generative {AI} Implementations in Enterprise Have No Measurable Impact on {P\&L}, Says {MIT}},
  year         = {2024},
  url          = {https://www.tomshardware.com/tech-industry/artificial-intelligence/95-percent-of-generative-ai-implementations-in-enterprise-have-no-measurable-impact-on-p-and-l-says-mit-flawed-integration-key-reason-why-ai-projects-underperform},
  organization = {Tom's Hardware}
}

@online{writer2024survey,
  author       = {{Writer}},
  title        = {Enterprise {AI} Adoption Survey: 72\% of Businesses Face Barriers to {GenAI} Adoption},
  year         = {2024},
  url          = {https://writer.com/blog/enterprise-ai-adoption-survey-press-release/},
  organization = {Writer.com}
}

@online{chromadb,
  title = {Chroma - the open-source embedding database.},
  url   = {https://github.com/chroma-core/chroma}
}

@online{pinecone,
  title = {The vector database for scale in production},
  url   = {https://www.pinecone.io/}
}

@online{qdrant,
  title = {High-Performance Vector Search at Scale},
  url   = {https://qdrant.tech/}
}

@online{openai2022embeddings,
  title = {New and improved embedding model},
  url   = {https://openai.com/blog/new-and-improved-embedding-model}
}

@online{mteb,
  title = {Embedding Leaderboard},
  url   = {https://huggingface.co/spaces/mteb/leaderboard}
}


% ===========================================================================
% UnstructuredIO and TABLE TRANSFORMER
% ===========================================================================
@misc{unstructured_io,
  author       = {{Unstructured.io}},
  title        = {Unstructured: The Ultimate ETL for LLMs},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/Unstructured-IO/unstructured}},
  note         = {Accessed: 2024}
}

@inproceedings{smock2021pubtables,
  title     = {PubTables-1M: Towards comprehensive table extraction from unstructured documents},
  author    = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4634--4645},
  year      = {2021}
}

% ==========================================================================
%% Bibliography entries for Chapter 2: Theoretical Background
%% Add these entries to your bibliography.bib file

@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@article{ji2023survey,
  title     = {Survey of hallucination in natural language generation},
  author    = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {12},
  pages     = {1--38},
  year      = {2023},
  publisher = {ACM}
}

@article{gao2024retrievalaugmented,
  title   = {Retrieval-augmented generation for large language models: A survey},
  author  = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal = {arXiv preprint arXiv:2312.10997},
  year    = {2024}
}

@inproceedings{reimers2019sentence,
  title     = {Sentence-BERT: Sentence embeddings using Siamese BERT-networks},
  author    = {Reimers, Nils and Gurevych, Iryna},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages     = {3982--3992},
  year      = {2019}
}

@article{wang2024multilingual,
  title   = {Multilingual E5 text embeddings: A technical report},
  author  = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal = {arXiv preprint arXiv:2402.05672},
  year    = {2024}
}

@article{malkov2020efficient,
  title     = {Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author    = {Malkov, Yu A and Yashunin, Dmitry A},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {42},
  number    = {4},
  pages     = {824--836},
  year      = {2020},
  publisher = {IEEE}
}

@inproceedings{karpukhin2020dense,
  title     = {Dense passage retrieval for open-domain question answering},
  author    = {Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {6769--6781},
  year      = {2020}
}

@article{ma2023hybrid,
  title   = {Hybrid retrieval with text augmentation for dense passage retrieval},
  author  = {Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy},
  journal = {arXiv preprint arXiv:2311.03572},
  year    = {2023}
}

@article{liu2024lost,
  title   = {Lost in the middle: How language models use long contexts},
  author  = {Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {12},
  pages   = {157--173},
  year    = {2024}
}

@inproceedings{devlin2019bert,
  title     = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages     = {4171--4186},
  year      = {2019}
}

@inproceedings{houlsby2019parameter,
  title        = {Parameter-efficient transfer learning for NLP},
  author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {2790--2799},
  year         = {2019},
  organization = {PMLR}
}

@inproceedings{li2021prefix,
  title     = {Prefix-tuning: Optimizing continuous prompts for generation},
  author    = {Li, Xiang Lisa and Liang, Percy},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)},
  pages     = {4582--4597},
  year      = {2021}
}

@article{dettmers2023qlora,
  title   = {QLoRA: Efficient finetuning of quantized LLMs},
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2023}
}

@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015}
}

@inproceedings{jiao2020tinybert,
  title     = {TinyBERT: Distilling BERT for natural language understanding},
  author    = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages     = {4163--4174},
  year      = {2020}
}

@misc{taori2023stanford,
  title        = {Stanford Alpaca: An instruction-following LLaMA model},
  author       = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  howpublished = {GitHub repository},
  year         = {2023},
  url          = {https://github.com/tatsu-lab/stanford_alpaca}
}

@inproceedings{wang2023selfinstruct,
  title     = {Self-instruct: Aligning language models with self-generated instructions},
  author    = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages     = {13484--13508},
  year      = {2023}
}

@article{zhou2023lima,
  title   = {LIMA: Less is more for alignment},
  author  = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2023}
}

@article{es2023ragas,
  title   = {RAGAS: Automated evaluation of retrieval augmented generation},
  author  = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  journal = {arXiv preprint arXiv:2309.15217},
  year    = {2023}
}

@article{zheng2023judging,
  title   = {Judging LLM-as-a-judge with MT-bench and chatbot arena},
  author  = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2023}
}

@inproceedings{houlsby2019parameter,
  title        = {Parameter-efficient transfer learning for NLP},
  author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle    = {International conference on machine learning},
  pages        = {2790--2799},
  year         = {2019},
  organization = {PMLR}
}