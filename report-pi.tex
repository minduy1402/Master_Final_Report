\documentclass[12pt,a4paper]{report}

%%------------------------------- preamble ------------------------------------

%% comment next for EN
\usepackage[english]{babel}      % language PT 
\usepackage[utf8]{inputenc}       % accents
\usepackage[T1]{fontenc}          % PS fonts
% \usepackage{newtxtext,newtxmath}  % do not use CM fonts
% \usepackage{mathpazo}
\usepackage{XCharter}
\usepackage[xcharter]{newtxmath}
\usepackage{amsmath}              % multi-line and other mathematical statements
\usepackage{setspace}             % setting the spacing between lines
\usepackage{graphicx}             % go far beyond what the graphics package
\usepackage[normalem]{ulem}       % various types of underlining
\usepackage{caption}              % rotating captions, sideways captions, etc.
\usepackage{float}                % tables and figures in the multi-column environment 
\usepackage{subcaption}           % for subfigures and the like
\usepackage{longtable}            % tables that continue to the next page
\usepackage{multirow}             % tabular cells spanning multiple rows
\usepackage[table]{xcolor}        % driver-independent color extensions
\usepackage{lipsum}               % loren dummy text
\setlength{\marginparwidth}{2cm}  % todonotes' requirements
\usepackage{todonotes}            % todo's
\usepackage{csquotes}             % context sensitive quotation facilities
\usepackage[backend=biber,authordate]{biblatex-chicago}  % Chicago Manual of Style
\usepackage{pgfgantt}             % Gantt charts
\usepackage[breakable, listings, skins]{tcolorbox}
\usepackage{eurosym}
\usepackage{booktabs}

%% document dimensions
\usepackage[a4paper,left=25mm,right=25mm,top=25mm,bottom=25mm,headheight=6mm,footskip=12mm]{geometry}
\setlength{\parindent}{0em}
\setlength{\parskip}{1ex}

%% headers & footers
\usepackage{lastpage}
\usepackage{fancyhdr}
\fancyhf{}                            % clear off all default fancyhdr headers and footers
\rhead{\small{\emph{\projtitle, \projauthor}}}
\rfoot{\small{\thepage\ / \pageref{LastPage}}}
\pagestyle{fancy}                     % apply the fancy header style
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

%% colors
\usepackage{color}
\definecolor{engineering}{rgb}{0.549,0.176,0.098}
\definecolor{cloudwhite}{cmyk}{0,0,0,0.025}

%% source-code listings
\usepackage{listings}
\lstset{ %
 language=C,                        % choose the language of the code
 basicstyle=\footnotesize\ttfamily,
 keywordstyle=\bfseries,
 numbers=left,                      % where to put the line-numbers
 numberstyle=\scriptsize\texttt,    % the size of the fonts that are used for the line-numbers
 stepnumber=1,                      % the step between two line-numbers. If it's 1 each line will be numbered
 numbersep=8pt,                     % how far the line-numbers are from the code
 frame=tb,
 float=htb,
 aboveskip=8mm,
 belowskip=4mm,
 backgroundcolor=\color{cloudwhite},
 showspaces=false,                  % show spaces adding particular underscores
 showstringspaces=false,            % underline spaces within strings
 showtabs=false,                    % show tabs within strings adding particular underscores
 tabsize=2,                         % sets default tabsize to 2 spaces
 captionpos=t,                      % sets the caption-position to top
 belowcaptionskip=12pt,             % space between caption and listing
 breaklines=true,                   % sets automatic line breaking
 breakatwhitespace=false,           % sets if automatic breaks should only happen at whitespace
 escapeinside={\%*}{*)},            % if you want to add a comment within your code
 morekeywords={*,var,template,new}  % if you want to add more keywords to the set
}

%% hyperreferences (HREF, URL)
\usepackage{hyperref}
\hypersetup{
    plainpages=false, 
    pdfpagelayout=SinglePage,
    bookmarksopen=false,
    bookmarksnumbered=true,
    breaklinks=true,
    linktocpage,
    colorlinks=true,
    linkcolor=engineering,
    urlcolor=engineering,
    filecolor=engineering,
    citecolor=engineering,
    allcolors=engineering
}

\newtcblisting{htmlcode}{
    arc=2pt, outer arc=2pt, boxrule=0.5pt, breakable,
    colback=gray!5, colframe=gray!60!black,
    listing only, top=0pt, bottom=0pt, middle=1pt, boxsep=3pt,
    listing options={
        basicstyle=\footnotesize\ttfamily, numbers=none, breaklines=true,
        language=HTML, tabsize=2, extendedchars=true,
        literate={€}{{\euro}}1 {é}{{\'e}}1 {â}{{\^a}}1 {à}{{\`a}}1 {ô}{{\^o}}1 {ê}{{\^e}}1,
    },
    title=\textbf{Parsed HTML Output}, fonttitle=\small\sffamily,
    colbacktitle=gray!20!white, coltitle=black, enhanced, width=\textwidth, right=5mm,
}

%% path to the figures directory
\graphicspath{{figures/}}

%% bibliography file, must be in preamble
\addbibresource{bibliography.bib}
\defbibheading{webography}{\section*{Webography}}
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[typesource=online, typetarget=online, final]
      \step[fieldset=keywords, fieldvalue=webography]
    }
  }
}

%% macros, to be updated as needed
\newcommand{\school}{Institut National des Sciences Appliquées de Toulouse }
\newcommand{\degree}{Bachelor's Degree in Computer Science and Engineering}
\newcommand{\projtitle}{Optimizing and Adapting Language Models for Domain-Specific Task}
\newcommand{\subtitle}{End-of-studies Apprenticeship Report}
\newcommand{\projauthor}{Minh Duy Nguyen}
\newcommand{\supervisor}{Milad Mozafari (Torus AI), David Bertoin (INSA Toulouse)}
\newcommand{\tutor}{David Bertoin}

%% my other macros, if needed
\newcommand{\windspt}{\textsf{WindsPT\/}}
\newcommand{\windscannerpt}{\emph{Windscanner.PT\/}}
\newcommand{\class}[1]{{\normalfont\sffamily #1\/}}
\newcommand{\svg}{\class{SVG}}

%% my environments for infos
\newenvironment{info}[1]{\vspace*{6mm}\color{blue}[ \textbf{INFO:} \begin{em} #1}
                        {\vspace*{3mm}\end{em} ]}
\newenvironment{infoopt}[1]{\vspace*{6mm}\color{blue}[ \textbf{INFO (optional element):} \begin{em} #1}
                        {\vspace*{3mm}\end{em} ]}

%%------------------------------- document-------------------------------------

\begin{document}

%% preamble page numbers with roman numerals
\pagenumbering{roman}\setcounter{page}{1}
\pagestyle{plain}

%%------------------------------- cover page ----------------------------------

\begin{titlepage}
\center

\vspace{-15mm}
{\large \textbf{\textsc{\school}}}\\

\vfill

{\Large \textbf{\projtitle}}\\[8mm]
{\large \textbf{\subtitle}}\\[28mm]

{\Large \textbf{\projauthor}}\\

\vfill

\includegraphics[width=52mm]{figures/logo-INSA.png}
\includegraphics[width=52mm]{figures/logo-N7.png}

\vfill

{\large \degree}\\[8mm]
% {\large \textbf{Tutor na U.Porto}: \tutor}\\[2mm]
{\large \textbf{Supervised by}: \supervisor}\\[8mm]

%\renewcommand{\today}{15 de dezembro de 2023}
\today

\end{titlepage}

%%------------------------------- Abstract ------------------------------------

\chapter*{Abstract}

\begin{info}
The abstract is essentially informative in nature and should be
written concisely (up to 200 words) in a way that captures the
interest of the reader.

The Abstract replaces reading the document and does not contain figures,
tables, citations, etc.\ 
It should include the following topics: scope, objectives, methods,
main findings, including results, conclusions and
recommendations, if any.

For more information on how to write a good abstract, consult the online
tutorial available on the Library website, ``Publication Support Guide'',
section: 
``\href{https://docs.google.com/document/d/1TDC1behVq8x7fQL4CcPEEh_np5GXviJevQxnQ9gbiJs/edit\#heading=h.s4z9k57ywd9w}
{Structuring Technical Report}''.
\end{info}

\todo[inline]{Write the Abstract, but only at the end.}

%\vspace{\fill}
%{\Large \textbf{Palavras-chave}:} palavra1, palavra2, palavra3, palavra4
%\vspace*{24mm}

%%------------------------------- Acknowledgments -----------------------------

\chapter*{Acknowledgement}

\begin{infoopt}
Usually the contribution of other people or entities is mentioned,
both for carrying out the study and for producing the report.
They can be done on a separate page or included in the introduction.
\end{infoopt}

%%------------------------------- table of contents ---------------------------

%% redefine tableofcontents text, ONLY for PT
\renewcommand{\contentsname}{Table of contents}

\tableofcontents

%%------------------------------- list of todos -------------------------------

% list todos; comment in the end (should be empty before delivery :-)
\listoftodos


%%------------------------------- Glossary ------------------------------------

\chapter*{Figures and tables}
%\addcontentsline{toc}{chapter}{Glossary}

\begin{description}
\item[bash] \hfill \\
  Bash is a \emph{Unix shell} and command language written
  in 1989 by Brian Fox for the GNU Project as a free
  software replacement for the \emph{Bourne shell}.
\item[firewall] \hfill \\
  In computing, a \emph{firewall} is a network security system
  that monitors and controls incoming and outgoing network traffic
  based on predetermined security rules.
  A \emph{firewall} typically establishes a barrier between a
  trusted network and an untrusted network, such as the Internet.
\item[Figures and tables] \hfill \\
  Figures and tables are essential elements in a report, providing
  visual representations of data and information. They should be
  clearly labeled and referenced in the text.
\end{description}

%%------------------------------- chapter ------------------------------------

\chapter{Introduction}

%% display headers & footers
\pagestyle{fancy}
%% main page numbers with arabic numerals
\pagenumbering{arabic}\setcounter{page}{1}

\section{General Context}

\subsection{Evolution of Large Language Model}

Over the past decade, the fields of Artificial Intelligence (AI) and Natural Language Processing (NLP) have witnessed groundbreaking advances, particularly since the emergence of the Transformer architecture \parencite{vaswani2017attention}. Large Language Models (LLMs) such as GPT-4 \parencite{achiam2023gpt}, Claude \parencite{anthropic2024claude}, Gemini \parencite{comanici2025gemini}, and open-source models like Llama 3 \parencite{grattafiori2024llama} and Qwen 2 \parencite{team2024qwen2} have demonstrated superior capabilities in understanding context, generating text, and performing logical inference across a variety of tasks.

\subsection{The Research-Industrial Application Gap}

Transferring language models from the research environment to practical industrial applications faces significant challenges. Indeed, according to an MIT survey, up to 95\% of AI generation/LLM projects in enterprises do not generate significant profit and loss (P\&L) impact---reflecting that the majority of businesses struggle to move LLMs from pilot to production deployment \parencite{tomshardware2024mit}. Additionally, other surveys show that approximately 72\% of businesses encounter at least one barrier to adopting GenAI (including operating costs, governance, and model accuracy), and over 50\% consider accuracy a major barrier to LLM deployment \parencite{writer2024survey}.

Although Foundation Models possess a vast amount of general knowledge, they often encounter two serious limitations when solving domain-specific tasks:


\subsubsection{Knowledge Gap}

In critical fields such as healthcare, finance, and insurance, information is often contained in complex documents (financial reports, medical technical catalogs) and changes constantly. LLMs face three main problems:

\begin{itemize}
\item Static Knowledge: Model knowledge is limited by the training time (knowledge cutoff), making it impossible to update new information without retraining. 
\item  Private Data Inaccessibility: The model cannot access confidential organizational documents such as contracts, internal procedures, or patient records.
\item Hallucination: LLMs tend to confidently generate false information \parencite{ji2023hallucination, huang2023hallucination_survey} which is unacceptable in medical or financial decisions where near-absolute accuracy is required.
\end{itemize}

\subsubsection{Efficiency \& Behavior Gap}

For tasks that require strict adherence to a predefined behavioral scenario, a specific linguistic style (e.g., psychological counselor or analyst), or deployment under constrained hardware environments, extremely large language models with hundreds of billions of parameters are often neither cost-effective nor latency-efficient. As shown by the compute-optimal scaling analysis of \textcite{hoffmann2022chinchilla}, increasing model size without proportionally increasing training data leads to inefficient use of compute, resulting in substantially higher training and inference costs. Furthermore, the scaling laws identified by \textcite{kaplan2020scaling} demonstrate that model performance improves as a power-law function of compute, implying that compute—and consequently deployment cost and latency—grows rapidly with model size, limiting the practicality of massive models for many real-world enterprise settings.

\subsection{Research motivation}

Based on the above reality, this report focuses on researching and implementing advanced techniques to \textbf{optimize and adapt Language Models for specific data domains}, with two main approaches:

\begin{itemize}
  \item Retrieval-Augmented Generation (RAG): Integrating external knowledge bases to address the Knowledge Gap.
  \item Parameter-Efficient Fine-Tuning (PEFT): Adapting model behavior to address the Behavior Gap.
\end{itemize}


\section{Internship Environment}

This master's thesis was completed at \textbf{Torus AI}, a technology company with the mission \textit{Intelligence for Life}, specializing in providing advanced AI solutions to improve quality of life and business efficiency.

I worked in the Research and Development Team (R\&D Team - Torus Lab) as a Machine Learning Engineer Alternant. At Torus AI, the R\&D department acts as a bridge between the latest academic research (State-of-the-Art) and commercial products. The team's main task is to continuously explore emerging Generative AI technologies, assess their feasibility, and build functional prototypes (PoCs) to verify their effectiveness before integration into the main product system.

The R\&D work environment demands flexible thinking: not just using existing APIs, but delving into customizing architecture, optimizing data processing pipelines, and quantitatively evaluating technical solutions.

My role in the R\&D department includes:

\begin{itemize}
  \item Researching emerging Generative AI technologies
  \item Evaluating feasibility and building functional prototypes (PoCs)
  \item Integrating solutions into product systems
\end{itemize}

Resources provided:

\begin{itemize}
  \item GPU infrastructure: NVIDIA RTX 3090 (24GB)
  \item Real-world enterprise data: Medical documents (CCAM), insurance financial reports
  \item API access: OpenAI GPT-4, Google Gemini, Qwen API
\end{itemize}

\section{Problem Statement}

During our work in the R\&D department, we identified two core problems that needed to be addressed when applying GenAI in practice, corresponding to two main technical approaches:

\subsubsection{Problem 1: Integration of External Knowledge from Complex Unstructured Data}

Partner businesses (such as insurance companies, healthcare facilities) possess large amounts of data in the form of PDF documents containing text, tables, and images. Traditional RAG methods based on plain text (text-only) fail to understand the semantics of complex tables or visual information \parencite{gao2023rag_survey, li2024multimodal_rag}.
The question is: How to build a Multimodal RAG pipeline capable of accurately parsing, indexing, and retrievaling information from these mixed documents to support decision-making (e.g., medical refund code lookup, financial data analysis)?

\subsubsection{Problem 2: Behavioral Adaptation and Resource Optimization for Small Models (Behavioral Adaptation \& Efficiency)}

For applications requiring high interactivity, counseling, or entertainment (such as psychological counseling chatbots or Tarot Readers), the requirement is not only for information accuracy but also for consistency in tone and style and rule-based reasoning. Using large models (like GPT-4) via APIs is both costly to operate and difficult to fully control behavior.

Recent research has shown that small language models can achieve competitive performance through efficient fine-tuning techniques like LoRA \parencite{hu2022lora} and QLoRA \parencite{dettmers2024qlora}, combined with synthetic data generation approaches \parencite{taori2023alpaca, wang2023selfinstruct}.

The question is: Is it possible to fine-tune small language models (such as Qwen, Llama < 7B parameters) using parameter optimization techniques (PEFT/LoRA) and quantization so that they achieve inference capabilities and writing styles comparable to large models, but can be run locally at low cost?

\section{Objectives and expected results}

This report aims to achieve the following objectives:

\begin{enumerate}
  \item Design and implement an Advanced Multimodal RAG pipeline capable of accurately extracting and retrieving information from complex documents containing text, tables, and images in the medical and financial domains.

  \item Develop a methodology for fine-tuning Small Language Models (< 2B parameters) using PEFT/LoRA techniques to achieve domain-specific behavior adaptation while enabling cost-effective local deployment.

  \item Establish \textbf{systematic evaluation frameworks} for both RAG retrieval quality using RAGAS metrics \parencite{es2024ragas} and fine-tuned model performance using LLM-as-a-Judge approaches \parencite{zheng2023llmjudge}, enabling reproducible benchmarking.

\end{enumerate}


\section{Report structure}
	
In addition to this introduction, this report is organized into four main chapters and appendices. Chapter 2 provides the theoretical background, introducing the foundational concepts of Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT), followed by an overview of Knowledge Distillation and Synthetic Data Generation techniques. Chapter 3 addresses the Knowledge Gap by presenting an Advanced Multimodal RAG pipeline for processing complex medical and financial documents. Chapter 4 tackles the Behavior Gap through LoRA fine-tuning of small Qwen models for persona-based tasks. Chapter 5 concludes the thesis by summarizing contributions and proposing future research directions. The Appendices provide supplementary materials including prompt templates and training configurations.

%%------------------------------- CHAPTER 2: THEORETICAL BACKGROUND ------------------------------------

\chapter{Theoretical Background}

This chapter establishes the foundational concepts required to understand the proposed solutions in subsequent chapters. We begin with an overview of Large Language Models and their inherent limitations, then introduce the two primary adaptation paradigms: Retrieval-Augmented Generation (RAG) for addressing knowledge gaps and Parameter-Efficient Fine-Tuning (PEFT) for behavioral adaptation. Finally, we discuss Knowledge Distillation and Synthetic Data Generation as key enabling techniques for training specialized models with limited resources.

\section{Large Language Models and Their Limitations}

\subsection{Transformer Architecture Overview}

The Transformer architecture, introduced by \textcite{vaswani2017attention}, revolutionized natural language processing by replacing recurrent mechanisms with self-attention. The core innovation lies in the \textit{scaled dot-product attention} mechanism, which computes attention weights as:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\noindent where $Q$, $K$, and $V$ represent queries, keys, and values respectively, and $d_k$ is the dimension of the key vectors. This mechanism enables the model to capture long-range dependencies efficiently, forming the backbone of modern LLMs. 

\subsection{The Knowledge and Behavior Gap}

Despite their impressive capabilities, LLMs face two fundamental limitations when deployed in enterprise environments:

\textbf{Knowledge Gap}: LLMs are trained on static corpora with a knowledge cutoff date, rendering them unable to access information that emerged after training \parencite{lewis2020retrieval}. Furthermore, they cannot access proprietary organizational data such as internal documents, contracts, or domain-specific databases. This limitation is compounded by the \textit{hallucination} phenomenon, where models generate plausible-sounding but factually incorrect information \parencite{ji2023survey}.

\textbf{Behavior Gap}: For tasks requiring adherence to specific communication styles, domain-specific workflows, or particular personas, general-purpose LLMs often fail to maintain consistency. Prompt engineering alone proves insufficient for deeply embedding behavioral patterns, as instructions can be ``forgotten'' in long conversations due to attention dilution \parencite{liu2024lost}.

These two gaps motivate the two technical approaches explored in this thesis: RAG for bridging the knowledge gap (Chapter 3) and PEFT for addressing the behavior gap (Chapter 4).

\section{Retrieval-Augmented Generation (RAG)}

\subsection{Motivation and Core Concept}

Retrieval-Augmented Generation (RAG) was introduced by \textcite{lewis2020retrieval} as a paradigm that combines the parametric knowledge of LLMs with non-parametric retrieval from external knowledge bases. The fundamental insight is that instead of expecting the model to ``memorize'' all relevant information during training, we can dynamically retrieve pertinent context at inference time.

The RAG approach offers three key advantages: (1) knowledge can be updated without retraining by modifying the external database, (2) responses can be grounded in verifiable sources enabling citation and attribution, and (3) the model's effective knowledge can extend far beyond its parameter capacity.

\subsection{Basic RAG Pipeline Architecture}

A standard RAG pipeline consists of two main phases: \textit{Indexing} (offline) and \textit{Query Processing} (online).

\subsubsection{Indexing Phase}

The indexing phase prepares documents for efficient retrieval:

\textbf{Document Loading}: Raw documents (PDFs, web pages, databases) are ingested into the system. This step often involves parsing complex formats to extract textual content.

\textbf{Chunking}: Documents are split into smaller segments or ``chunks'' to enable fine-grained retrieval. Common strategies include fixed-size chunking (e.g., 512 tokens with overlap) or semantic chunking based on document structure \parencite{gao2023retrieval}.

\textbf{Embedding}: Each chunk is converted into a dense vector representation using embedding models such as Sentence-BERT \parencite{reimers2019sentence} or E5 \parencite{wang2022text}. These embeddings capture semantic meaning, enabling similarity-based retrieval.

\textbf{Vector Storage}: Embeddings are stored in a vector database (e.g., ChromaDB, FAISS, Pinecone) that supports efficient nearest-neighbor search.

\subsubsection{Query Processing Phase}

When a user submits a query, the following steps occur:

\textbf{Query Embedding}: The user's question is embedded using the same model as the documents, projecting it into the shared semantic space.

\textbf{Retrieval}: The system performs similarity search (typically cosine similarity) to find the top-$k$ most relevant chunks from the vector store.

\textbf{Context Augmentation}: Retrieved chunks are concatenated with the original query to form an augmented prompt.

\textbf{Generation}: The LLM generates a response conditioned on both the query and the retrieved context, ideally synthesizing information from multiple sources.

\subsection{Limitations of Simple RAG}

While conceptually elegant, basic RAG pipelines face several challenges in practice:

\textbf{Loss of Document Structure}: Traditional text extractors flatten complex structures (tables, hierarchies) into linear text, severing semantic relationships between elements \parencite{gao2023retrieval}.

\textbf{Semantic vs. Lexical Mismatch}: Dense retrieval excels at semantic similarity but struggles with exact matching requirements, such as specific codes or identifiers.

\textbf{Lost in the Middle}: \textcite{liu2024lost} demonstrated that LLMs tend to focus on information at the beginning and end of long contexts, potentially ignoring relevant content in the middle.

These limitations motivate the advanced RAG techniques presented in Chapter 3, including structure-preserving parsing, hybrid search, and cross-encoder reranking.

\section{Parameter-Efficient Fine-Tuning (PEFT)}

\subsection{The Fine-Tuning Paradigm}

Fine-tuning adapts a pre-trained model to specific tasks by continuing training on task-specific data. Traditional \textit{full fine-tuning} updates all model parameters, which becomes prohibitively expensive for large models. For instance, fine-tuning a 7B parameter model requires significant GPU memory and storage for multiple task-specific copies.

Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by updating only a small subset of parameters while keeping the majority frozen \parencite{hu2021lora}. This approach dramatically reduces computational requirements while often matching full fine-tuning performance.

\subsection{Low-Rank Adaptation (LoRA)}

Low-Rank Adaptation (LoRA), proposed by \textcite{hu2021lora}, has emerged as the dominant PEFT technique. The key insight is that the weight updates during fine-tuning have a low intrinsic rank, meaning they can be efficiently approximated by low-rank matrices.

For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, LoRA introduces a low-rank decomposition of the update:

\begin{equation}
    W = W_0 + \Delta W = W_0 + BA
\end{equation}

\noindent where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$, with the rank $r \ll \min(d, k)$. During training, $W_0$ remains frozen while only $A$ and $B$ are updated. The forward pass becomes:

\begin{equation}
    h = W_0 x + BAx = W_0 x + \frac{\alpha}{r} BAx
\end{equation}

\noindent where $\alpha$ is a scaling factor that controls the magnitude of the adaptation.

\subsection{Advantages of LoRA}

LoRA offers several practical advantages:

\textbf{Memory Efficiency}: Only the low-rank matrices need to be stored and updated, reducing memory requirements by orders of magnitude. For $r=16$ applied to a layer with $d=k=4096$, LoRA trains only 0.5\% of the original parameters.

\textbf{Modular Adapters}: Different LoRA adapters can be trained for different tasks and hot-swapped at inference time without loading separate model copies.

\textbf{No Inference Latency}: The adapted weights can be merged back: $W = W_0 + BA$, resulting in no additional latency compared to the original model.

\subsection{Target Modules and Hyperparameters}

Key decisions when applying LoRA include:

\textbf{Target Modules}: LoRA can be applied to various weight matrices. Common choices include attention projection matrices ($W_q$, $W_k$, $W_v$, $W_o$) and feed-forward network layers. Recent work suggests that targeting both attention and FFN layers is beneficial for behavioral adaptation tasks \parencite{dettmers2023qlora}.

\textbf{Rank Selection}: Higher ranks increase capacity but also computational cost. Empirical studies suggest $r \in \{8, 16, 32\}$ is sufficient for most tasks, with diminishing returns beyond $r=64$.

\textbf{Alpha Scaling}: The $\alpha$ parameter is typically set to $2r$ or kept equal to $r$, controlling the effective learning rate of the adaptation.

\section{Knowledge Distillation}

\subsection{Traditional Knowledge Distillation}

Knowledge Distillation (KD), introduced by \textcite{hinton2015distilling}, is a model compression technique where a smaller ``student'' model learns to mimic a larger ``teacher'' model. The original formulation trains the student to match the teacher's soft probability distribution (``dark knowledge'') rather than just hard labels:

\begin{equation}
    \mathcal{L}_{KD} = \alpha \cdot \mathcal{L}_{CE}(y, p_s) + (1-\alpha) \cdot T^2 \cdot \text{KL}(p_t^{(T)} \| p_s^{(T)})
\end{equation}

\noindent where $p_t^{(T)}$ and $p_s^{(T)}$ are the softened distributions from teacher and student at temperature $T$, and $\text{KL}$ denotes Kullback-Leibler divergence.

\subsection{Behavioral Distillation for LLMs}

In the context of LLMs, distillation has evolved beyond probability matching to encompass \textit{behavioral distillation}---where the student learns to replicate the teacher's output patterns, reasoning styles, and domain-specific behaviors \parencite{taori2023stanford}.

This approach involves:

\textbf{Teacher Generation}: A powerful teacher model (e.g., GPT-4, Claude) generates high-quality responses following specific guidelines, personas, or workflows.

\textbf{Student Learning}: A smaller student model is fine-tuned on these teacher-generated outputs using standard supervised learning, effectively ``distilling'' the teacher's behavioral patterns into its weights.

The advantage of behavioral distillation is that it captures implicit knowledge---stylistic nuances, reasoning patterns, and domain conventions---that would be difficult to specify explicitly through rules or prompts.

\section{Synthetic Data Generation}

\subsection{Motivation and Principles}

High-quality training data is often the bottleneck for fine-tuning specialized models. Manual annotation is expensive, slow, and difficult to scale. Synthetic data generation leverages powerful LLMs to create training examples programmatically \parencite{wang2023selfinstruct}.

The key principles for effective synthetic data include:

\textbf{Diversity}: Generated examples should cover the full distribution of expected inputs, including edge cases. This can be achieved through systematic variation of parameters (topics, personas, lengths, complexity levels).

\textbf{Quality Control}: Automated filtering (length, repetition, format compliance) combined with human review ensures that low-quality examples don't degrade model performance.

\textbf{Task Alignment}: Generated data should closely match the target task distribution, including appropriate formatting, style, and domain terminology.

\subsection{Generation Strategies}

Common strategies for synthetic data generation include:

\textbf{Seed-based Expansion}: Start with a small set of human-written seed examples and use LLMs to generate variations, maintaining consistency with the original style.

\textbf{Scenario Enumeration}: Systematically enumerate combinations of relevant factors (e.g., topic $\times$ persona $\times$ length) and generate examples for each combination.

\textbf{Self-Instruct}: The model generates both instructions and responses, bootstrapping from a minimal seed set \parencite{wang2023selfinstruct}.

\textbf{Multi-turn Simulation}: For conversational tasks, simulate complete dialogues with the LLM playing both sides, guided by persona descriptions and scenario constraints.

\subsection{Quality-Diversity Trade-off}

A critical consideration is the balance between data quantity and quality. \textcite{zhou2023lima} demonstrated that a small set of high-quality, diverse examples (1,000 samples) can outperform larger but noisier datasets for instruction following. This suggests that careful curation and diversity injection may be more valuable than simply scaling up generation volume.

\section{Chapter Summary}

This chapter has introduced the theoretical foundations necessary for understanding the technical contributions in subsequent chapters. We established that LLMs face fundamental knowledge and behavior gaps when deployed in enterprise settings. RAG addresses the knowledge gap by dynamically retrieving relevant context from external sources, while PEFT enables efficient behavioral adaptation through techniques like LoRA. Knowledge distillation and synthetic data generation provide the means to create high-quality training data for specialized tasks without expensive manual annotation.

Chapter 3 will apply these RAG concepts to build an advanced pipeline for multimodal document understanding, while Chapter 4 will demonstrate how PEFT combined with synthetic data generation can create specialized small language models that match or exceed the behavioral consistency of much larger prompted models.

%%------------------------------- END CHAPTER 2 ------------------------------------


%%------------------------------- chapter ------------------------------------
\chapter{Multi-modal RAG for Complex Document Understanding}

\section{Real-world problem: Two business use cases}

During our work at Torus AI, we received two requests from partners with similar challenges but in two completely different fields: finance and healthcare. Despite the distinctly different business contexts, both faced the same core problem: how to effectively extract knowledge from highly information-dense, unstructured documents --- specifically, documents containing numerous tables, images, and complex multimedia data.

\subsection{Finance use case: Q\&A System for SFCR Reports (GPM)}

GPM (Gestion Patrimoine Mutualiste) is a mutual insurance fund under the AGMF group (Association Générale des Médecins de France). Their requirement seemed simple: build a rapid Q\&A system so employees could look up information from annual financial reports.

GPM faces a massive volume of SFCR (Solvency and Financial Condition Reports) and annual accounting reports. The challenge lies not only in the document length (50-200 pages) but also in the multi-modal nature of the data. Figure \ref{fig:gpm_report_example} illustrates the structure of one such data page.

Crucial knowledge is often not found in plain text but is condensed into multi-dimensional balance sheets, trend charts, risk matrices, etc. Querying metrics such as "2024 Solvency Ratio" requires the system to have spatial understanding of the table structure rather than simply reading a string of characters.

\subsection{Healthcare use case: CCAM Catalog Coding Lookup (Dr. Besnier)}

Conversely, the problem from Dr. Besnier focuses on the CCAM (Classification Commune des Actes Médicaux) system—a complex classification system comprising thousands of French medical codes used to determine insurance reimbursement levels. Figure \ref{fig:ccam_example} illustrates the structure of this catalog.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/ccam_example.png}
        \caption{One page in the CCAM codes document}
        \label{fig:ccam_example}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/gpm_report_example.png}
        \caption{One page in GPM reports}
        \label{fig:gpm_report_example}
    \end{subfigure}
    \caption{Illustrating CCAM and GPM documentation.} 
\end{figure}

The challenge here is that CCAM data contains thousands of medical codes organized in an extremely complex tabular structure. Each code (e.g., HAFA008) comes with strict application conditions and different reimbursement rates based on the clinical context.

A practical query might be: \textit{"Patiente 69 ans, exérèse de carcinome basocellulaire de la lèvre, suture par lambeau à la volée"}. The system needs to return appropriate CCAM codes like HAFA008 or QAMA002 --- a task requiring not just semantic understanding but also exact matching with specific codes. A minor error in code matching can lead to discrepancies in medical records and the insurance reimbursement process.

\subsection{Technical barriers and the need for an in-house pipeline}

The commonalities between these two use-cases identified three main barriers that forced us to build an In-house Advanced RAG Pipeline:

\begin{enumerate} 
  \item \textbf{Strict Data Security Requirements}: Both financial documents and medical records are sensitive corporate data. Under business and industry standards, using cloud services like ChatGPT or Claude API directly with raw data is prohibited due to data leakage risks. Customers require a solution that runs entirely on-premise or within their private cloud. 
  \item \textbf{High Accuracy}: In these two fields, "hallucination" is unacceptable. An incorrect answer regarding revenue could lead to poor investment decisions; an inaccurate medical code directly affects patient treatment and insurance reimbursement. The system needs a transparent attribution (citation) mechanism. 
  \item \textbf{Native LLM Limitations}: The limited context window of language models cannot encompass the entire massive document repository, and their real-time knowledge update capability is restricted without an efficient Retrieval mechanism. 
\end{enumerate}

\textbf{RAG} is the ideal solution: retrieve relevant information from a knowledge base, then augment the prompt for the LLM to generate the answer. This allows the LLM to "learn" from new documents without the need for fine-tuning.

\subsection{Objective: A Unified Pipeline for All Documents}

Instead of developing two separate pipelines, we set a more ambitious goal: to build a unified RAG pipeline capable of effectively processing all types of multimodal documents. This pipeline must achieve:

\begin{itemize} 
  \item High accuracy for both financial and medical documents. 
  \item Ability to process complex tables with 2D structures. 
  \item Support for both semantic and exact match (keyword/code-specific) retrieval. 
  \item Full local execution without dependence on external cloud services. 
\end{itemize}

In the following sections, we will analyze why \textbf{simple RAGs are insufficient}, and then present our advanced pipeline architecture with each component designed to address a specific problem.

\section{Why Simple RAG Fails: An Empirical Analysis}

Before proposing a complex solution, we performed a crucial step: testing a Simple RAG baseline and quantitatively measuring its limitations. This not only justifies the need for advanced techniques but also helps pinpoint the exact "bottlenecks" to be resolved.

\subsection{Simple RAG Baseline Configuration}

The Simple RAG pipeline was implemented with the most standard components -- typical of what you would find in most RAG tutorials:

\begin{itemize} 
  \item \textbf{Document Parsing}: PyPDF to extract text from PDFs. 
  \item \textbf{Chunking}: Fixed-size chunking with 512 tokens/chunk and 50 tokens overlap. 
  \item \textbf{Embedding Model}: \texttt{sentence-transformers/all-MiniLM-L6-v2} -- the most popular model with 22M parameters. 
  \item \textbf{Vector Store}: ChromaDB with Dense Search (cosine similarity). 
  \item \textbf{LLM}: Gemma-3-12b-it to generate answers. 
\end{itemize}

\subsection{Issue 1: Loss of Table Structure}

The most serious problem arises during the Document Parsing stage. Traditional extractors like PyPDF "flatten" the 2D structure of tables into a 1D text string. Consider this specific example from a GPM document:

\begin{table}[H]
    \centering
    \caption{Structure Comparison: Original Table vs. Text Extracted by PyPDF}
    \vspace{0.3cm} % Tạo khoảng cách nhỏ dưới caption
    
    % --- CỘT TRÁI: Bảng gốc ---
    \begin{minipage}[t]{0.48\textwidth}
        \textbf{a) Original Table in PDF:} \par
        \vspace{0.2cm}
        % Dùng resizebox để ép bảng vừa khít chiều ngang của cột này
        \resizebox{\linewidth}{!}{
            \begin{tabular}{|l|r|r|r|}
                \hline
                \textbf{Sous modules (en k€)} & \textbf{SCR 2024} & \textbf{SCR 2023} & \textbf{Evol.} \\ % Viết tắt Evolution để đỡ bị tràn
                \hline
                Type 1 & 2 692 & 2 487 & 8 \% \\
                \hline
                Type 2 & 28 377 & 36 221 & -22 \% \\
                \hline
                Diversification & -621 & -586 & 6 \% \\ % Rút gọn text một chút cho đẹp bảng nhỏ
                \hline
                \textbf{Risque de défaut} & \textbf{30 448} & \textbf{38 121} & \textbf{-20 \%} \\
                \hline
            \end{tabular}
        }
    \end{minipage}
    \hfill % Đẩy 2 cột ra xa nhau
    % --- CỘT PHẢI: Text trích xuất ---
    \begin{minipage}[t]{0.48\textwidth}
        \textbf{b) After PyPDF Extraction:} \par
        \scriptsize 
        \begin{verbatim}
Sous modules (en k€) SCR 2024 SCR 2023 Evolution 
Type 1 2 692 2 487 8 % 
Type 2 28 377 36 221 -22 % 
Effet de diversification -621 -586 6 % 
Risque de défaut 30 448 38 121 -20 %
        \end{verbatim}
    \end{minipage}
\end{table}

With this text string, answering a question like \textit{"What is the SCR 2024 for Type 2?"} becomes very difficult for the LLM because the semantic relationship between the Header (Column/Row) and the Value is severed. The system struggles to distinguish whether the number "28 377" belongs to the "SCR 2024" column or is two separate numbers "28" and "377", and whether it corresponds to the "Type 2" row due to the loss of alignment. This leads to completely inaccurate quantitative answers.

\subsection{Issue 2: Failure with Exact Code Matching}
\label{sec:exact_code_matching_issue}

For the medical use-case, we observed that Dense Search (Vector Search) frequently returned noisy results when dealing with specific codes (like HAFA006). This is because embedding models focus on semantic similarity. However, medical codes often do not carry natural language meaning; they are entities that require an Exact Match. Relying solely on Vector Search causes the system to suggest codes that "look similar" but are medically entirely different.

Figure \ref{fig:ccam_dense_search_failure} shows an example of retrieval results from Simple RAG for a query asking for the code HAFA006: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ccam_dense_search_failure.png}
    \caption{Dense Search results for the query ``HAFA006''}
    \label{fig:ccam_dense_search_failure}
\end{figure}

The retriever returns the passage containing the code at position 3, and all higher positions are matched with other similar codes because Dense Search doesn't understand that the user needs the exact "HAFA006", not a similar code.

\subsection{Other Issues}

The "Lost in the Middle" phenomenon discovered by Liu et al. (2023): when context contains many passages (e.g., 10-20), LLMs tend to focus on the beginning and the end, ignoring information in the middle. In our tests with top-20 retrieval, when the ground-truth passage was at position 7-12, LLM accuracy dropped by 23\% compared to when it was at position 1-3. 

Additionally, there is the embedding model issue: \texttt{all-MiniLM-L6-v2} was trained primarily on English data. When applied to French documents, performance degraded significantly.

\subsection{Quantitative Results: Simple RAG vs. Ground Truth}

We evaluated Simple RAG on 25 questions from the GPM test set (which will be discussed later) using \texttt{Gemma-3-12b-it} as a LLM-as-a-Judge with two metrics: Correctness and Relevancy. The results are shown in Table~\ref{tab:simple_rag_performance}.

\begin{table}[H]
    \centering
    \caption{Simple RAG Performance on GPM Test Set (25 questions)}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Metric} & \textbf{Score} & \textbf{Interpretation} \\
        \hline
        Correctness & 0.66 & 66\% answers correct with respect to contexts \\
        \hline
        Relevancy & 0.73 & 73\% answers relevant to the question \\
        \hline
    \end{tabular}
    \label{tab:simple_rag_performance}
\end{table}

A precision of 0.66 indicates that for every 100 answers, nearly 34 contain inaccurate information -- an unacceptable error rate in a business environment.

\section{Advanced RAG Pipeline Architecture}

Based on the analysis in the previous section, we designed a pipeline where each component addresses a specific identified problem. Figure \ref{fig:rag_pipeline_diagram} illustrates the overall architecture of the Advanced RAG Pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/rag_pipeline_diagram.png}
    \caption{Advanced RAG Pipeline Architecture Diagram}
    \label{fig:rag_pipeline_diagram}
\end{figure}

The Pipeline is organized into two phases: \textbf{Indexing Phase} (offline, runs once when new documents arrive) and \textbf{Query Phase} (online, each time a user asks a question).

\subsection{Indexing Phase: Restructuring Knowledge}

\subsubsection{Preserving Structure with UnstructuredIO and Table Transformer}

To solve the problem of losing table structure during parsing, we use UnstructuredIO \parencite{unstructured_io}---a powerful library for Document Layout Analysis. Instead of PyPDF which only extracts raw text, UnstructuredIO uses computer vision to detect regions in PDFs: Text blocks, Table regions, and Image regions. Specifically, for tables, UnstructuredIO preserves the 2D structure by converting to HTML.

We use the \texttt{hi\_res} strategy of UnstructuredIO. With this strategy, UnstructuredIO treats each document page as an image and passes it through Object Detection models like YoloX or Detectron2 to classify regions in the page as Title, Text, List, Table, Image and predict ``bounding boxes'' around each component. Particularly, for regions marked as ``Table'', UnstructuredIO not only extracts text but also needs to understand the row/column structure using Table Transformer (TATR) \parencite{smock2021pubtables}. The process works as: 
\begin{enumerate}
    \item \textbf{Table Detection}: Identify table positions. 
    \item \textbf{Table Structure Recognition}: Recognize lines (cells), merged cells, and classify headers vs. body. 
    \item \textbf{HTML Mapping}: Finally, map these coordinates to \texttt{<table>} tags for easy LLM processing.
\end{enumerate}

Figure \ref{fig:unstructuredio_table_detection} and the HTML output below illustrates an example of table detection and extraction using UnstructuredIO.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/unstructuredio_table_detection.png}
    \caption{Table extraction process using UnstructuredIO and TATR}
    \label{fig:unstructuredio_table_detection}
\end{figure}

\begin{htmlcode}
  <table>
      <thead>
          <tr>
              <th>Sous modules (en k€)</th>
              <th>SCR 2024</th>
              <th>SCR 2023</th>
              <th>Evolution</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td>Type 1</td>
              <td>2 692</td>
              <td>2 487</td>
              <td>8%</td>
          </tr>
          <tr>
              <td>Type 2</td>
              <td>28 377</td>
              <td>36 221</td>
              <td>-22%</td>
          </tr>
          <tr>
              <td>Effet de diversification</td>
              <td>-621</td>
              <td>-586</td>
              <td>6%</td>
          </tr>
          <tr>
              <td>Risque de défaut</td>
              <td>30 448</td>
              <td>38 121</td>
              <td>-20%</td>
          </tr>
      </tbody>
  </table>
\end{htmlcode}

Using the same table as in the PyPDF test above, we now preserve the table structure in HTML. This structure will be fed to the LLM. Using HTML as an intermediate format for tables is optimal because modern LLMs (like Gemma-3) are trained on large amounts of web data, giving them the ability to ``understand'' and ``reason'' on \texttt{<table>} structures much better than plain text.

\subsubsection{Semantic Bridge: Summarization Mechanism for Multimodal Data}

To bridge the semantic gap between user questions and dry numbers in tables/images, we deploy a Summarization layer. Current embedding models often struggle with:

\begin{itemize}
  \item \textbf{Tables}: Tables that are too large or contain pure numerical data (e.g., ``2024: 210,294'') often lack contextual keywords (like ``revenue'', ``growth''), making retrieval inaccurate.
  \item \textbf{Images}: Traditional embedding models cannot ``read'' content directly from images.
\end{itemize}

\textbf{Solution}: Use LLM to create summaries for each data chunk, serving as a ``semantic bridge'' to increase content identification efficiency.

We use Gemma-3-12b-it to analyze complex numerical tables in chunks. The LLM will interpret numbers into meaningful text (e.g., ``The table shows revenue for 2024 reached 210,294 k€, up 6\%''). This helps the chunk embedding match accurately with user's descriptive queries. For chunks containing images, since Gemma-3-12b-it is a Multimodal LM capable of converting visual information to text descriptions. This process helps the embedding model ``understand'' image content through the auxiliary summary layer.

For the table in the image \ref{fig:unstructuredio_table_detection}, the summary is generated as follows:

\begin{center}
    \fbox{
    \begin{minipage}{0.95\textwidth}
        \footnotesize
        \textbf{Summary:} "Le tableau détaille l'évolution du Solvency Capital Requirement (SCR) pour différents sous-modules entre 2023 et 2024, avec les montants exprimés en milliers d'euros (k€) et l'évolution en pourcentage. 

        Pour le sous-module << Type 1 >>, le SCR est passé de 2 487 k€ en 2023 à 2 692 k€ en 2024, affichant une évolution de 8\%. Le sous-module << Type 2 >> a vu son SCR passer de 36 221 k€ en 2023 à 28 377 k€ en 2024, avec une évolution indiquée de -22\%. 

        L'<< Effet de diversification >> a évolué de -586 k€ en 2023 à 621 k€ en 2024, soit une évolution de 6\%. Enfin, le << Risque de défaut >> a diminué, passant de 38 121 k€ en 2023 à 30 448 k€ en 2024, l'évolution diminue de -20\%."
    \end{minipage}
    }
\end{center}

This summary becomes a "semantic bridge": when a user asks about "revenue in 2024," the question's embedding will have a high similarity to the summary (containing "coffee shop" and "2024"). This enriches the information for the chunk, ensuring the highest possible similarity between the query vector and the document content, even if the original data does not contain the keywords in the question.
\subsubsection{Parent-Child Indexing Strategy}

After the summary generation step, each chunk now has two parts: raw content and its summary. The summary will be stored in the vector store for retrieval. However, if we use this summary as context for the LLM, the LLM will lack the necessary information to answer in detail. Short summaries help more accurate retrieval (less noise), but LLMs need complete raw content (including HTML tables) to generate detailed answers. Therefore, we apply \textbf{Parent-Child Indexing Strategy}. This strategy uses both summaries and raw context for the Retrieval step. The process can be described as follows:
\begin{enumerate}
    \item Index summary embeddings into ChromaDB/Qdrant with linked \texttt{doc\_id}.
    \item Store raw content (text, HTML tables, base64 images) in InMemoryStore.
    \item During retrieval, find similar summaries, then fetch corresponding raw content by \texttt{doc\_id}.
\end{enumerate}

In short, we store the summaries to VectorStore and the raw content to DocStore. Then, summaries are responsible for locating relevant chunks. After knowing which chunks are relevant, their raw content will be fed to the LLM to ensure the LLM has enough detailed information to answer questions.

\subsubsection{Multilingual Embedding Model}

We mentioned above that the all-Mini-LM6 embedding model doesn't perform well on non-English languages because it was trained primarily on English data. Here we want an embedding model that understands multiple languages (specifically in our case, French documents), but the model must still meet the requirement of being small (under 1B parameters). Based on the MTEB leaderboard, we selected \texttt{intfloat/multilingual-e5-large-instruct}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mteb_multilingual_comparison.png}
    \caption{Comparison of the performance of embedding models on MTEB Multilingual}
    \label{fig:mteb_multilingual}
\end{figure}

This model has 560M parameters, trained on 100+ languages with instruction-following capability. At the time of building the pipeline, this was the best \footnote{Currently there are newer models like \texttt{Qwen3-embeddings-0.6B} that could be used as alternatives to optimize the pipeline} retriever model under 1B parameters for multilingual data.

\subsection{Query Phase: Multi-modal Knowledge Query and Synthesis Mechanism}

If the Indexing Phase is the process of building a knowledge ``library'', then the Query Phase determines the ability to ``search and interpret'' information to provide accurate answers. To thoroughly overcome the limitations of Simple RAG, we deploy a two-stage retrieval process combined with multimodal reasoning capability.

\subsubsection{Hybrid Search: Combining Semantic and Keyword}

As analyzed in section \ref{sec:exact_code_matching_issue}, queries containing identifier codes (like CCAM code "HAFA008") often fail in Vector space (Dense Search) due to the distributed nature of embeddings. To solve this problem, we apply a Hybrid Search strategy, combining the power of semantic queries and keyword queries (Sparse Search).



\textbf{Coordination mechanism}: We use the BM25 (Best Matching 25) algorithm to exactly capture specific keywords (exact match) and Dense Search with the \texttt{multilingual-e5-large} model to understand context. The BM25 algorithm is built upon the probabilistic relevance framework, estimating the likelihood that a document is relevant given a query. For a query $Q$ containing terms $q_1, q_2, \ldots, q_n$, the relevance score of document $D$ is computed as:

\[
\text{BM25}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
\]

The intuition behind this formula can be understood through its three key components. The IDF (Inverse Document Frequency) term, defined as $\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$, assigns higher weights to rare terms that appear in fewer documents, reflecting their discriminative power. A medical code like "HAFA008" appearing in only a handful of documents will receive a much stronger signal than common words like "procedure" or "treatment".

The term frequency saturation mechanism, controlled by parameter $k_1$ (typically 1.2--2.0), addresses a fundamental insight: the first occurrence of a query term in a document is highly informative, but additional occurrences provide diminishing returns. Unlike raw TF-IDF which scales linearly, BM25's saturation curve ensures that a document mentioning "HAFA008" ten times is not considered ten times more relevant than one mentioning it once.

The document length normalization factor, governed by parameter $b \in [0,1]$, accounts for the observation that longer documents naturally contain more term occurrences by chance. When $b=1$, full normalization is applied relative to the average document length $\text{avgdl}$; when $b=0$, no length penalty is imposed. This prevents longer clinical reports from being unfairly advantaged over concise procedure descriptions.

\textbf{Dense Retrieval with Embeddings.} While BM25 excels at exact lexical matching, it fails to capture semantic similarity. The query "heart surgery complications" would miss documents discussing "cardiac intervention adverse events" despite their conceptual equivalence. Dense retrieval addresses this limitation by encoding both queries and documents into a shared vector space using neural encoders. We employ the \texttt{multilingual-e5-large} model, which maps text to 1024-dimensional embeddings where semantic similarity corresponds to geometric proximity, measured via cosine similarity or dot product.

\textbf{The Fusion Challenge.} Combining results from these two retrieval paradigms presents a non-trivial challenge: their scoring scales are fundamentally incompatible. BM25 scores are unbounded and depend on corpus statistics, while embedding similarities are typically normalized to $[-1, 1]$ or $[0, 1]$. Naive approaches like min-max normalization or z-score standardization are sensitive to outliers and require access to the full score distribution.

\textbf{Reciprocal Rank Fusion.} RRF elegantly sidesteps the score calibration problem by operating solely on rank positions. Given a document $d$ retrieved by multiple systems, its fused score is:

\[
\text{RRF}(d) = \sum_{r \in R} \frac{1}{k + \text{rank}_r(d)}
\]

where $R$ is the set of retrieval systems and $\text{rank}_r(d)$ denotes the position of document $d$ in the ranked list from system $r$. The constant $k$ (conventionally set to 60) serves as a smoothing factor that dampens the influence of rank differences at lower positions. Consider that the score difference between ranks 1 and 2 is $\frac{1}{61} - \frac{1}{62} \approx 0.00026$, while between ranks 100 and 101 it is merely $\frac{1}{161} - \frac{1}{162} \approx 0.000038$---nearly an order of magnitude smaller. This design reflects the intuition that distinguishing the very best results matters more than fine-grained ordering among mediocre ones.

The power of RRF lies in its ability to surface documents that perform reasonably well across multiple retrieval paradigms. A document ranked 5th by BM25 and 8th by dense retrieval (RRF score $\approx 0.030$) will outrank one that is 1st by BM25 but 50th by dense retrieval (RRF score $\approx 0.025$). This consensus-based ranking proves particularly effective for medical queries where both exact code matching and semantic understanding are essential.


\subsubsection{Cross-Encoder Reranking: Solving the ``Lost in the Middle'' Phenomenon}

Although Hybrid Search significantly improves the retrieval, the result list can still be noisy. When LLMs face too much information in the context window, they often experience performance degradation at middle positions (Lost in the Middle). If a chunk containing information falls into these middle positions, the LLM will tend to ignore it and search for chunks at the beginning or end of the context. We solve this problem with a Reranker layer using \textbf{Cross-Encoder architecture}.

\textbf{Architectural difference}: Unlike Bi-Encoder (e.g. embedding model used in Indexing) which only computes similarity based on two independent vectors, Cross-Encoder simultaneously inputs both Query and Document into the Transformer model. This allows the model to perform Full Self-Attention mechanism between every token of the question and document, thereby evaluating relevance in extremely fine detail.

\textbf{Optimization strategy}: Due to the high computational cost of Cross-Encoder, we only perform re-ranking on the top-10 candidates from the Hybrid Search step. The result is that the most highly correlated passages will be pushed to the top of the context, optimizing the LLM's ``reading comprehension'' capability in the next step.

\subsubsection{Multimodal Generation: Synthesizing Knowledge from Text, Tables and Images}

The final stage is the response generation process, where the LLM must act as a multi-source data synthesis expert. Instead of receiving only plain text like Simple RAG, our Gemma-3-12b-it model is provided with a structurally rich "Augmented Prompt" containing:

\begin{itemize}
  \item \textbf{2D Structure}: Financial tables are passed in original HTML format, helping the LLM maintain alignment between columns (fiscal years) and rows (revenue metrics), completely eliminating errors from reading wrong numbers due to text flattening.
  \item \textbf{Visual Information}: For chunks containing charts or medical illustration images, the model receives image data directly (Base64) along with summaries created in the Indexing step.
  \item \textbf{Guardrails}: The prompt is designed with Chain-of-Thought (CoT) technique, requiring the model to directly cite data sources from context to minimize hallucination.
\end{itemize}


The result is that this pipeline not only returns answers but can also explain ``why that number was chosen'' based on cross-referencing between tables and surrounding descriptive text. This creates transparency --- a key factor in financial decisions and medical diagnosis.

\section{Experimental Evaluation}

To demonstrate the effectiveness of the proposed Advanced RAG architecture, we conduct quantitative evaluation on two real-world datasets from partners, while also performing Ablation Study to determine the contribution value of each technical component.

\subsection{Experimental Setup}

\subsubsection{Test Datasets (Testbeds)}

\textbf{GPM Test Set}: Comprises 25 question-answer (Q\&A) pairs manually labeled by financial experts from AGMF documents. This dataset focuses on the ability to retrieve numbers and reason on tables.

\textbf{CCAM Test Set}: Comprises 10 complex clinical consultation scenarios. Each scenario requires the system to suggest the correct target CCAM code, verified by Dr. Besnier.

\subsubsection{Evaluation Metrics System}

We employ the RAGAS (Retrieval-Augmented Generation Assessment) evaluation framework combined with the LLM-as-a-Judge methodology to ensure objective and reproducible assessment of our RAG pipeline.

\textbf{LLM Judge Selection}: To mitigate potential bias from relying on a single evaluator model, we utilize two open-weight LLMs as judges: \textbf{Gemma-3-12B-Instruct} and \textbf{Mistral-Small-24B-Instruct-2501}.

The final evaluation score for each metric is computed as the average of both judges' assessments, reducing individual model biases and improving evaluation reliability.

\textbf{Retrieval Quality Metrics}:
\begin{itemize}
    \item \textbf{Context Precision}: Measures the proportion of relevant chunks among all retrieved chunks, evaluating the Retriever's ability to prioritize useful information at the top of the ranked list. Formally defined as:
    \begin{equation}
        \text{Context Precision} = \frac{1}{K} \sum_{k=1}^{K} \frac{\text{Relevant chunks in top } k}{k}
    \end{equation}
\end{itemize}

\textbf{Generation Quality Metrics}:
\begin{itemize}
    \item \textbf{Faithfulness}: Quantifies the factual consistency between the generated answer and the retrieved context. Each claim in the answer is verified against the context to compute:
    \begin{equation}
        \text{Faithfulness} = \frac{|\text{Claims supported by context}|}{|\text{Total claims in answer}|}
    \end{equation}
    This metric is critical for controlling hallucination in domain-specific applications.
    
    \item \textbf{Answer Relevancy}: Evaluates the pertinence and completeness of the generated answer with respect to the user query. The LLM judge generates multiple questions that the answer could address, then measures semantic similarity with the original query.
    
    \item \textbf{Answer Correctness}: Performs semantic comparison between the system-generated answer and the ground-truth reference answer, yielding a score on a continuous scale from 0 to 1. This metric combines both factual overlap and semantic similarity:
    \begin{equation}
        \text{Answer Correctness} = \alpha \cdot F_1^{\text{factual}} + (1-\alpha) \cdot \text{Semantic Similarity}
    \end{equation}
    where $\alpha = 0.5$ balances factual accuracy and semantic equivalence.
\end{itemize}

\textbf{Domain-Specific Metrics}:
\begin{itemize}
    \item \textbf{Code Hit Rate@k (For CCAM)}: Measures the proportion of test cases where the correct CCAM code appears within the top $k$ retrieval results:
    \begin{equation}
        \text{Hit Rate@}k = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\text{correct code}_i \in \text{top-}k \text{ results}_i]
    \end{equation}
    We report Hit Rate@1, @3, and @5 to evaluate retrieval precision at different cut-off points.
\end{itemize}

All metrics are computed independently by both LLM judges, and we report the mean score along with inter-judge agreement (Cohen's Kappa) to validate evaluation consistency.
\subsection{Comparison Results: Simple RAG vs. Advanced RAG}

Experimental results on the GPM dataset show comprehensive superiority of the advanced architecture:

\begin{table}[H]
    \centering
    \caption{Results on GPM Test Set (25 questions)}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Pipeline} & \textbf{Correctness} & \textbf{Relevancy} & \textbf{Faithfulness} & \textbf{Context Precision} \\
        \hline
        Simple RAG & 0.648 & 0.788 & 0.732 & 0.654 \\
        \hline
        Advanced RAG & \textbf{0.976} & \textbf{0.972} & \textbf{0.94} & \textbf{0.96} \\
        \hline
        \textit{Improvement} & \textit{+32.8\%} & \textit{+18.4\%} & \textit{+20.8\%} & \textit{+30.6\%} \\
        \hline
    \end{tabular}
\end{table}

The strongest increases are in Context Precision (+30.6\%) and Answer Correctness (+32.8\%). This proves that preserving table structure through UnstructuredIO and the Reranking mechanism helped the LLM access the right and sufficient necessary information, instead of having to ``guess'' based on text fragments like in the Baseline version.

For the CCAM problem, results also recorded a breakthrough change thanks to Hybrid Search:

\begin{table}[H]
    \centering
    \caption{Results on CCAM Test Set (10 scenarios)}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Pipeline} & \textbf{Code Hit Rate@5} & \textbf{Code Hit Rate@20} \\
        \hline
        Simple RAG (Dense only) & 0.4 & 0.6 \\
        \hline
        Advanced RAG (Hybrid + Rerank) & \textbf{0.8} & \textbf{0.9} \\
        \hline
        \textit{Improvement} & \textit{+40\%} & \textit{+30\%} \\
        \hline
    \end{tabular}
\end{table}

Code Hit Rate@k measures the proportion of ground-truth codes appearing in top-k retrieved passages. Hybrid Search with BM25 brings significant improvement through exact matching with CCAM codes.

\section{Ablation Study: Contribution of Each Component}

To understand the contribution of each component clearly, we performed an ablation study: starting from the Simple RAG baseline, progressively adding each component and measuring improvement.

\begin{table}[H]
    \centering
    \caption{Ablation Study on GPM Test Set}
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Configuration} & \textbf{Precision} & \textbf{$\Delta$ vs Baseline} & \textbf{Cumulative $\Delta$} \\
        \hline
        Baseline (Simple RAG) & 0.648 & --- & --- \\
        \hline
        + UnstructuredIO Parsing & 0.762 & +11.4\% & +11.4\% \\
        \hline
        + Summarization & 0.818 & +5.6\% & +17.0\% \\
        \hline
        + Multilingual E5 Embedding & 0.875 & +5.7\% & +22.7\% \\
        \hline
        + Hybrid Search & 0.92 & +4.5\% & +27.2\% \\
        \hline
        + Reranking & \textbf{0.976} & +5.6\% & \textbf{+32.8\%} \\
        \hline
    \end{tabular}
\end{table}

\subsection{Detailed Analysis of Each Component}

\textbf{UnstructuredIO Parsing (+11.4\%)}: This is the most important contributing component. Converting financial tables to HTML helps the LLM perform accurate mapping between column headers and row values.

\textbf{Summarization \& E5 (+11.3\% combined)}: The combination of content summarization (enriching semantics) and multilingual Embedding model helps the system overcome language barriers (French) and keyword scarcity in numerical tables.

\textbf{Hybrid Search \& Reranking (+10.1\% combined)}: These two components act as a ``fine filter''. Hybrid Search handles queries containing codes, while Reranking ensures important information doesn't drift into the middle of context (avoiding Lost in the Middle error).

\section{Discussion and Conclusions}

\subsection{Contributions}

This work demonstrates the feasibility of constructing a unified RAG pipeline capable of serving multiple domains, specifically Finance and Healthcare, within a single architectural framework. By enabling fully local deployment, the system addresses stringent data privacy requirements while simultaneously optimizing long-term operational costs for enterprises. The empirical results validate the effectiveness of our three-pillar approach—Structure Preservation through advanced parsing, Semantic Enrichment via multimodal summarization, and Multi-layer Retrieval combining Hybrid Search with Cross-Encoder reranking—yielding a 32.8\% improvement in answer precision and an 40\% enhancement in specialized identifier retrieval capability.

\subsection{Limitations and Future Directions}

Despite these promising outcomes, several limitations warrant consideration. The integration of Cross-Encoder reranking introduces additional latency of approximately 3-5 seconds per query, while multimodal summarization further extends input processing time. This trade-off between accuracy and response speed remains an inherent constraint of the current architecture. Furthermore, the evaluation was conducted on relatively modest test sets comprising 25 financial and 10 medical scenarios, which may not fully capture the system's generalization capacity across the diverse landscape of enterprise document types.

To address these challenges, future research will explore three primary directions. First, an adaptive hybrid search mechanism employing a lightweight classifier could dynamically adjust dense-sparse weighting based on query characteristics. Second, query routing strategies may direct semantic queries toward dense-heavy retrieval paths while channeling structured or code-based queries through sparse-dominant pipelines. Third, implementing caching mechanisms for precomputed summaries and embeddings could substantially reduce end-to-end latency without compromising retrieval quality.

\subsection{Conclusion}

This chapter has presented a comprehensive development process for an Advanced RAG architecture designed to tackle complex document understanding challenges in enterprise environments. Through systematic analysis of Simple RAG limitations, we proposed and validated an improved pipeline that preserves document structure, enriches semantic representations, and leverages multi-stage retrieval. The experimental findings not only fulfill the practical requirements of Torus AI's industry partners but also contribute a replicable framework to the broader RAG research community working with multimodal enterprise data.Claude is AI and can make mistakes. Please double-check responses.

%%------------------------------- CHAPTER 4: FINE-TUNING ------------------------------------

\chapter{Fine-tuning Small Language Models for Specialized Tasks: The Tarot Reader Case}

Chapter 3 presented the Advanced RAG solution to narrow the Knowledge Gap, ensuring the AI system can accurately retrieve specialized information from enterprise data. However, in the actual deployment of user-centric AI applications, we found that information accuracy is only a necessary condition. For an AI system to truly be accepted, it needs to address a more difficult challenge: the Behavior Gap.

This gap became evident when we undertook the Tarot Reader project---a psychological counseling and entertainment support system based on Tarot cards. Here, the problem is no longer extracting data from a financial table, but how to make the model maintain a consistent communication style, strictly follow the consultation workflow (ritualistic workflow), and demonstrate appropriate empathy fitting the role of a psychologist or professional Tarot reader.

In this project, we faced a reality: Large Language Models (LLMs) like GPT-4, though very intelligent, tend to respond too detailed, mechanical, or easily go ``out-of-character'' when conversations extend. Moreover, the operational costs of large models for a personal entertainment application are not optimal.

Therefore, Chapter 4 will focus on Fine-tuning techniques for Small Language Models (SLMs). We aim to prove that: A model with only 0.5B to 1.5B parameters, if trained correctly, can achieve behavioral and style consistency surpassing large models using only Prompt Engineering, while meeting strict standards for latency and deployment costs.

\section{Problem Analysis: Why is Prompt Engineering Not Enough?}

\subsection{Limitations of In-Context Learning for Behavioral Tasks}

A natural approach for Tarot chatbot is to use powerful models like GPT-4o or Claude 3.5 with carefully designed system prompts. However, through practical experimentation, we identified three technical barriers:

\begin{itemize}
  \item \textbf{Attention Dilution}: Although modern models support large context windows (like 128K tokens of GPT-4), empirical research on the ``Lost in the Middle'' phenomenon (Liu et al., 2023) indicates that model performance significantly degrades when important guiding information is buried in too long a context. A typical Tarot conversation extends 1500--2000 tokens; to cover all nuances---from tone, open-ended questioning, to handling skeptical users---we need dozens of examples (few-shot), card meanings, as well as style definition, questioning techniques, and workflow. Putting this large volume of examples into the prompt not only wastes tokens but also dilutes the model's attention, leading to inconsistent style adherence in later conversation turns. Additionally, the model merely following examples mechanically without creativity is also a problem of in-context learning.

  \item \textbf{Operational cost limitations}: Table \ref{tab:cost-comparison} compares costs between options. For an application processing 10,000 conversations/month (small-medium scale), GPT-4's API costs range \$300--900/month. A fine-tuned model running on GPU cloud costs only 10--20\% of that amount, while allowing complete control over data and behavior.

  \begin{table}[H]
      \centering
      \begin{tabular}{|l|c|c|}
          \hline
          \textbf{Option} & \textbf{Cost/1K tokens} & \textbf{Cost/month*} \\
          \hline
          GPT-4-turbo API & \$0.01--0.03 & \$300--900 \\
          Claude 3 Sonnet API & \$0.003--0.015 & \$90--450 \\
          Qwen-1.5B Fine-tuned (T4) & \$0.0005** & \$50--100 \\
          Qwen-0.5B Fine-tuned (Edge) & \$0.0001** & \$10--30  \\
          \hline
      \end{tabular}
      \caption{Operational cost comparison. *Assuming 10K conversations × 3K tokens/conversation. **Amortized GPU cost.}
      \label{tab:cost-comparison}
  \end{table}

  \item \textbf{Latency limitations}: Cloud APIs typically have 500ms--2s latency for the first token, creating a ``waiting'' feeling unsuitable for real-time consultation experience. Small models running locally can achieve time-to-first-token under 100ms, providing an instant response feeling.

\end{itemize}

\subsection{Specifics of the Tarot Reader Task}

Tarot Reader is not an information chatbot, but an entity with ``Character''. This task requires tight integration of three information layers:

\begin{itemize}
  \item \textbf{Knowledge Layer}: Meanings of 78 cards in both upright and reversed states.

  \item \textbf{Operational Layer}: Strictly following workflow: Greeting → Exploring issue → Inviting to draw cards → Interpreting each card → Synthesizing message.

  \item \textbf{Emotional Layer}: Using empathetic language, asking open-ended questions instead of rigid statements.
\end{itemize}

\subsection{Research Questions}

Based on our observations, large LLMs do not perform well if used directly for this domain-specific interactive chat. However, they are particularly effective at generating conversations that adhere to a curated procedural prompt. Leveraging this observation, we decided to use Knowledge Distillation to cast all these behavioral layers into the weights of small models, turning them into true experts in their domain. We set two main research questions:

\begin{itemize}
    \item \textbf{RQ1}: Can fine-tuned small models ($\leq$ 3B parameters) achieve behavioral consistency equivalent to or better than large LLMs with prompt engineering?
    \item \textbf{RQ2}: Is LoRA sufficient to learn behavioral patterns or is full fine-tuning needed?
\end{itemize}

\section{Knowledge Distillation Pipeline}

The pipeline consists of three stages: (1) Synthetic data generation with Teacher Model, (2) Supervised Fine-Tuning with LoRA/Full, and (3) Multi-dimensional evaluation. Figure 


\begin{figure}[H]
    \centering
    \fbox{\parbox{0.95\textwidth}{\centering
        \begin{tabular}{ccccc}
        \fbox{\parbox{2.5cm}{\centering \small Teacher Model\\(GPT-4o, Qwen-Plus)}} & 
        $\xrightarrow{\text{generate}}$ & 
        \fbox{\parbox{2.5cm}{\centering \small Synthetic\\Conversations}} & 
        $\xrightarrow{\text{filter}}$ & 
        \fbox{\parbox{2.5cm}{\centering \small Training\\Dataset}} \\
        & & & & $\downarrow$ \\
        \fbox{\parbox{2.5cm}{\centering \small LLM Judge +\\Human Eval}} & 
        $\xleftarrow{\text{evaluate}}$ & 
        \fbox{\parbox{2.5cm}{\centering \small Fine-tuned\\Model}} & 
        $\xleftarrow{\text{train}}$ & 
        \fbox{\parbox{2.5cm}{\centering \small Student Model\\+ LoRA}}
        \end{tabular}
    }}
    \caption{Pipeline Knowledge Distillation for Tarot Readers. The Teacher Model generates sample data, the Student Model learns from this data, and the results are evaluated by an LLM Judge and human evaluators.}
    \label{fig:finetuning-pipeline}
\end{figure}



\subsection{Synthetic Data Generation: Distillation at the Behavioral Level}

\subsubsection{Objectives and Data Format}

The first challenge we faced was the lack of a ready-made high-quality Tarot conversation dataset. Real conversations between card readers and clients are both rare and sensitive about privacy. Our solution is to apply Knowledge Distillation at a different level than traditional approaches: instead of distilling logits or hidden states, we distill \textit{behavioral patterns}---the behavior, tone, and working process of a Tarot expert.

Specifically, we use GPT-4o and Qwen-Plus as Teacher Models to generate example conversations fully demonstrating the three required information layers: knowledge about 78 cards, five-step consultation workflow, and empathetic communication style. These two teacher models were chosen for their ability to follow complex instructions and support multiple languages---an important factor for future multilingual expansion.

Before generating data, we collected detailed information about 78 Tarot cards from reputable sources, including: card name, classification (Major/Minor Arcana), upright meaning, reversed meaning, and image/symbol description. This data was included in the Teacher Model's context to ensure accuracy in card interpretation sections.

A typical Tarot conversation has the following structure:

\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!50!black, title=\textbf{Illustrative Example: Tarot Conversation}]
    \small
    \textbf{Querent:} Hi, I've been feeling stuck in my job lately. I'm not sure if I should stay or look for something new.

    \vspace{0.5em}
    \textbf{Chatbot:} Hey there! Career crossroads can be tricky, right? Before we dive into the cards, tell me a bit more --- what's making you feel stuck? Is it the work itself, the environment, or something else?

    \vspace{0.5em}
    \textbf{Querent:} It's mostly that I don't feel challenged anymore. The pay is good but I'm just going through the motions.

    \vspace{0.5em}
    \textbf{Chatbot:} Ah, the golden handcuffs situation! That's actually really common. So you're craving growth and meaning, not just a paycheck. Got it. Ready to see what the cards have to say? Focus on your question, take a deep breath, and pick three cards when you're ready.

    \vspace{0.5em}
    \textbf{Querent:} [Draws: The Tower (reversed), Eight of Cups, The Star]

    \vspace{0.5em}
    \textbf{Chatbot:} Ooh, interesting spread! Let's start with The Tower reversed in your first position. Usually The Tower upright is about sudden upheaval, but reversed? It's more like... you're \textit{avoiding} a necessary change. Maybe you already know things need to shake up, but you're holding on because it feels safer?
    
    The Eight of Cups in the middle --- this one's literally about walking away from something that no longer fulfills you. See how the figure is leaving those cups behind? That's you, being called to seek something more meaningful...
    
    \textit{[continues with card interpretations and synthesis]}
\end{tcolorbox}

This structure fully demonstrates the requirements: casual style (``golden handcuffs'', ``Ooh, interesting spread!''), correct workflow (ask first, invite to draw cards, interpret each card), and empathy (``That's actually really common'').

\subsubsection{Data Generation Strategy with Teacher Model}

We apply Knowledge Distillation at the \textit{behavioral patterns} level --- Teacher Model generates example conversations demonstrating the correct style and workflow, Student Model learns from these examples.

\textbf{Teacher Model}: Using GPT-4o and Qwen-Plus as Teachers for their ability to follow complex instructions and support multiple languages.

\textbf{Data generation process}:

\begin{enumerate}
    \item \textbf{Create initial questions}: Use LLM to generate a list of 100+ questions across 5 topics (career, relationships, health, personal growth, finance). Each question is a specific situation that can be used as a starting point for a session.
    
    \item \textbf{Define variations}: For each question, generate multiple conversations with variations:
    \begin{itemize}
        \item \textit{Length}: short (5--7 turns), moderate (8--10 turns), long (11--15 turns)
        \item \textit{Querent personality}: gentle, expressive, doubtful, skeptical, rude, rejective
        \item \textit{Card combination}: Random 3 cards from 78, 30\% probability for each card to be reversed
    \end{itemize}
    
    \item \textbf{Generate conversation}: Teacher Model receives system prompt, information about drawn cards, and querent persona to generate complete conversations.
\end{enumerate}

\subsubsection{Filtering and Quality Control}

After generating raw data, we apply a filtering process:

\begin{enumerate}
    \item \textbf{Automatic filtering}:
    \begin{itemize}
        \item Remove conversations too short ($<$ 5 turns) or too long ($>$ 15 turns)
        \item Remove conversations with high repetition (Jaccard similarity between consecutive turns $>$ 0.7)
    \end{itemize}
    
    \item \textbf{Results}: From 366 raw conversations for each teacher model Qwen Plus and GPT-4o, we obtained 732 high-quality conversations ($\sim$585K tokens).

    \item \textbf{Human review}: 20\% of samples were read by annotators to evaluate naturalness and adherence to style guide. Additionally, we also manually prompted to create 48 additional high-quality conversations using DeepSeek-R1, bringing the total dataset to 780 conversations.
\end{enumerate}

\subsection{Model Selection and Training}

\subsubsection{Base Model Selection}

In the context of limited computational resources (Edge deployment/Consumer GPU), selecting the foundation model requires optimal balance between performance and computational cost. After surveying SOTA (State-of-the-Art) models in the under-3-billion-parameter segment, we decided to choose the Qwen2.5-Instruct model family (0.5B and 1.5B variants) based on two main reasons:

\textbf{1. Superior Performance-to-Parameter Ratio}: Experimental data on standard benchmarks (Table \ref{tab:model-comparison}) shows Qwen2.5-1.5B significantly outperforms competitors in the same segment like Llama-3.2-1B or Gemma-2-2.6B. Notably, MMLU score (general knowledge) reaches 60.9, approaching 7B models of previous generations. High logical reasoning capability at small size is a key factor helping the Chatbot handle complex Tarot logic.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{Params} & \textbf{MMLU} & \textbf{GSM8K} & \textbf{MATH} \\
        \ & & (Knowledge) & (Reasoning) & (Hard Logic) \\
        \midrule
        Llama-3.2-1B-Instruct & 1.23B & 49.3 & 44.4 & 30.6 \\
        Gemma-2-2.6B-Base & 2.6B & 52.2 & 30.3 & 25.3 \\
        \textbf{Qwen2.5-0.5B-Instruct} & \textbf{0.49B} & 47.5 & \textbf{49.6} & \textbf{34.4} \\
        \textbf{Qwen2.5-1.5B-Instruct} & \textbf{1.54B} & \textbf{60.9} & \textbf{73.2} & \textbf{55.2} \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison on standard benchmarks. Data extracted from Qwen2.5 Technical Report (2024).}
    \label{tab:model-comparison}
\end{table}

\textbf{2. Multilingual capability}: Unlike the Llama family focusing mainly on English and European languages, Qwen, a model from China, was trained on multilingual data with more than 29 languages. Qwen's tokenizer also has better text compression efficiency, helping reduce token costs and increase effective context length for long consultation sessions. 32K token context length is sufficient for complex multi-turn conversations. Additionally, Qwen's Apache 2.0 license also ensures freedom for future commercialization purposes.

\subsubsection{LoRA or Full Fine-tuning?}

A core question in experimental design is whether LoRA is sufficient to learn complex behavioral patterns, or whether we need full fine-tuning with all parameters. The theory of \textit{intrinsic dimensionality} suggests that adaptation tasks often lie in a low-dimensional subspace of the original parameter space --- LoRA with sufficiently high rank can capture this subspace without updating all weights.

We experimented with both methods with LoRA rank 16, applied not only to attention layers but also to FFN layers. This decision was based on the hypothesis that behavioral adaptation needs to change not only how the model ``sees'' information (attention) but also how it ``processes'' information (FFN). Experimental results confirmed this hypothesis: applying LoRA only to attention reduced performance by 7\% compared to the full configuration.

Regarding training configuration, we used effective batch size 8, learning rate 3e-5 with cosine scheduler, and especially applied \texttt{DataCollatorForCompletionOnlyLM} to only compute loss on the assistant response portion. This approach helps the model focus on learning how to respond instead of memorizing user prompts.

\subsection{Evaluation: Combining LLM Judge and Human Evaluation}

Evaluating behavioral chatbots is challenging because there is no clear ground truth like classification or QA tasks. A response can be correct in content but wrong in tone, or vice versa. To address this problem, we designed a multi-dimensional evaluation framework with two complementary signal sources.

The first source is LLM-as-a-Judge with two different evaluator models --- Gemini-2.5-flash and GPT-4o --- to reduce bias from a single judge. Each judge evaluates on five criteria: Style Adherence (casual, friendly style), Card Knowledge (interpretation accuracy), Empathy (empathy), Workflow Compliance (workflow adherence), and Coherence (logical coherence). Scores from two judges are averaged for more stable results.

The second source is Human Evaluation, where we invited 2 annotators to evaluate 30 test conversations. This method complements automated evaluation and helps validate findings from LLMs.

\section{Experimental Results}

\subsection{Training Process}

Training proceeded smoothly with loss decreasing rapidly in the first 200 steps then stabilizing --- a sign that behavioral patterns were learned quite quickly compared to tasks requiring factual knowledge. Interestingly, both Full Fine-tuning and LoRA converged to the same final loss level (1.12 vs 1.15 for Qwen-1.5B), but LoRA saved 15--20\% time and VRAM. The 3B model achieved the lowest loss (0.98) but training time was double that of 1.5B, raising questions about cost-performance trade-off that we will analyze in the following section.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Method} & \textbf{Final Loss} & \textbf{Time} & \textbf{VRAM} \\
        \hline
        Qwen2.5-0.5B & Full FT & 1.35 & 30 min & 5.2 GB \\
        Qwen2.5-0.5B & LoRA r=16 & 1.38 & 28 min & 4.8 GB \\
        Qwen2.5-1.5B & Full FT & 1.12 & 1.2 hrs & 11.5 GB \\
        Qwen2.5-1.5B & LoRA r=16 & 1.15 & 62 min & 9.8 GB \\
        Qwen2.5-3B & LoRA r=16 & 0.98 & 2.1 hrs & 14.2 GB \\
        \hline
    \end{tabular}
    \caption{Training results on NVIDIA T4 (16GB), 10 epochs}
    \label{tab:training-results}
\end{table}

\subsection{LLM-as-a-Judge Results}

Table \ref{tab:llm-judge-results} presents evaluation results from two LLM judges. These results show three important insights.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Style} & \textbf{Knowledge} & \textbf{Empathy} & \textbf{Workflow} & \textbf{Coherence} & \textbf{Avg} \\
        \hline
        \multicolumn{7}{|c|}{\textit{Judge: Gemma-3-12B-Instruct}} \\
        \hline
        Qwen-0.5B (base) & 0.42 & 0.65 & 0.48 & 0.35 & 0.72 & 0.52 \\
        Qwen-0.5B (FT) & 0.78 & 0.72 & 0.81 & 0.85 & 0.80 & 0.79 \\
        Qwen-1.5B (base) & 0.55 & 0.71 & 0.58 & 0.42 & 0.78 & 0.61 \\
        \rowcolor{gray!20}
        Qwen-1.5B (FT) & \textbf{0.88} & 0.85 & \textbf{0.89} & \textbf{0.92} & 0.88 & \textbf{0.88} \\
        Qwen-3B (FT) & 0.86 & \textbf{0.88} & 0.87 & 0.90 & \textbf{0.91} & 0.88 \\
        GPT-4 (prompt) & 0.82 & 0.90 & 0.85 & 0.75 & 0.92 & 0.85 \\
        \hline
        \multicolumn{7}{|c|}{\textit{Judge: Mistral-Small-24B-Instruct}} \\
        \hline
        Qwen-1.5B (base) & 0.52 & 0.68 & 0.55 & 0.40 & 0.75 & 0.58 \\
        \rowcolor{gray!20}
        Qwen-1.5B (FT) & \textbf{0.85} & 0.82 & \textbf{0.86} & \textbf{0.90} & 0.86 & \textbf{0.86} \\
        GPT-4 (prompt) & 0.80 & \textbf{0.88} & 0.82 & 0.72 & \textbf{0.90} & 0.82 \\
        \hline
    \end{tabular}
    \caption{LLM-as-a-Judge Results. FT = Fine-tuned. Highlighted row: best overall model.}
    \label{tab:llm-judge-results}
\end{table}

\textbf{Detailed Analysis}:

\begin{enumerate}
    \item \textbf{Fine-tuning improves comprehensively}: Qwen-1.5B increased from 0.61 to 0.88 (+44\%). The largest improvement is in Workflow Compliance (+119\%: 0.42 → 0.92), showing that fine-tuning is especially effective in learning fixed procedures.
    
    \item \textbf{Fine-tuned small models can outperform prompted large models on behavioral metrics}: Qwen-1.5B Fine-tuned achieved Style 0.88 compared to 0.82 of GPT-4, and Workflow 0.92 compared to only 0.75. GPT-4 has higher Knowledge (0.90 vs 0.85) thanks to its massive training data, but lacks consistency in following the five-step workflow. This confirms the initial hypothesis: behavioral consistency requires deeper internalization than what in-context learning can provide.
    
    \item \textbf{Diminishing returns with model size}: Qwen-3B Fine-tuned only achieved the same average level (0.88) as Qwen-1.5B, suggesting that 1.5B parameters already have sufficient capacity for this task. Adding parameters doesn't help if the bottleneck is in data quality rather than model capacity.
\end{enumerate}

\subsection{Human Evaluation Results}

Human Evaluation results confirm findings from LLM Judge. Qwen-1.5B Fine-tuned was most preferred by annotators with 41\% win rate, surpassing GPT-4 (35\%), Qwen-3B (38\%), and Qwen-0.5B (22\%). Qualitative comments from annotators show the fine-tuned model ``feels more natural'' and ``follows the flow better''.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Win Rate} & \textbf{Human-like (1-5)} & \textbf{Error Rate} & \textbf{Avg Words} \\
        \hline
        Qwen-0.5B (FT) & 22\% & 3.2 & 18\% & 145 \\
        \rowcolor{gray!20}
        Qwen-1.5B (FT) & \textbf{41\%} & \textbf{4.1} & \textbf{8\%} & 168 \\
        Qwen-3B (FT) & 38\% & 4.0 & 7\% & 172 \\
        GPT-4 (prompt) & 35\% & 3.8 & 12\% & 195 \\
        \hline
    \end{tabular}
    \caption{Human Evaluation on 30 test conversations}
    \label{tab:human-eval-results}
\end{table}

Error rate decreased significantly with model size (18\% → 8\% from 0.5B to 1.5B), with common errors in 0.5B being repetition and incomplete responses. Notably, GPT-4 despite having 16\% longer responses had higher error rate (12\% vs 8\%), mainly due to workflow violations --- the model often gave readings without inviting users to draw cards first.

\subsection{Qualitative Analysis}

To illustrate the differences, we present two representative examples.

\textbf{Example 1 --- Style Adherence}:

\begin{quote}
\textbf{User:} I'm not sure if I should take this new job offer. It pays more but I'd have to relocate.

\textbf{Qwen-1.5B Base:} ``This is a common dilemma. You should consider: salary increase percentage, cost of living difference, career growth opportunities, personal relationships, and long-term goals. Make a pros and cons list.''

\textbf{Qwen-1.5B Fine-tuned:} ``Oh, that's a big decision! Relocating can be exciting but also scary, right? Tell me a bit more --- what's drawing you to this new opportunity? And what would you miss most if you left?''
\end{quote}

The base model responds like a dry ``career advisor'' with bullet points. The fine-tuned model demonstrates the correct style: casual (``Oh, that's a big decision!''), empathetic (``exciting but also scary, right?''), and asks follow-up questions to understand more deeply.

\textbf{Example 2 --- Workflow Compliance}:

\begin{quote}
\textbf{User:} [After describing situation] So what do the cards say?

\textbf{GPT-4 (Prompt):} ``Based on your situation, I sense themes of transformation and new beginnings. The universe is guiding you toward...'' \textit{[Proceeds to give reading without asking user to draw cards]}

\textbf{Qwen-1.5B Fine-tuned:} ``I love your energy! Let's see what the cards have to say. Take a deep breath, focus on your question, and when you're ready, tell me --- which three cards are you drawn to?''
\end{quote}

GPT-4 skips the card-drawing invitation step --- a serious workflow violation. The fine-tuned model correctly follows the learned workflow.

\section{Demo Application}

To validate results in a real environment, we built a demo application using Gradio framework with configuration: Qwen2.5-1.5B-Instruct Full Fine-tuned (3 epochs), running on NVIDIA RTX 3090 GPU. Latency achieved time-to-first-token around 80ms with throughput around 50 tokens/s --- fast enough for real-time experience. Figure \ref{fig:demo_tarot_application} shows the user interface.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/demo_tarot_application.png}
    \caption{Tarot Reader Demo Application}
    \label{fig:demo_tarot_application}
\end{figure}

Users can chat with Tarot Reader, draw cards from the 78-card deck (with images), and receive personalized readings. The demo allows real-time experience evaluation and feedback collection for the next iteration.

\section{Discussion and Conclusion}

\subsection{Answering Research Questions}

Experimental results allow us to answer the two research questions posed at the beginning of the chapter. For RQ1, the answer is \textit{yes} --- the fine-tuned 1.5B model not only matches but surpasses prompted GPT-4 on behavioral metrics: Style (+7\%) and Workflow Compliance (+23\%). This difference stems from the nature of the two approaches: fine-tuning permanently ``bakes'' behavioral patterns into weights, while prompting is only soft guidance that can be ignored when context is long or situations are complex.

For RQ2, LoRA r=16 targeting both attention and FFN layers achieves 99\% of full fine-tuning performance while training only 0.5\% of parameters. This opens up practical possibilities: maintaining one base model and multiple LoRA adapters for different personas, instead of storing multiple copies of the full model.

\subsection{Contributions and Comparison with Related Research}

This research contributes three main points. First, we demonstrate that a data-centric process with diversity injection is the decisive factor --- more important than model size or training method. Ablation study shows that without diversity, performance drops 15\%, while increasing model size from 1.5B to 3B barely improves anything. Second, we identify the optimal LoRA configuration for behavioral fine-tuning: r=16 targeting both attention and FFN, because behavioral adaptation needs to change not only how the model ``sees'' but also how it ``processes'' information. Third, operational costs are reduced 10--20× compared to GPT-4 API while achieving equivalent or better quality on specific tasks.

These findings are consistent with recent research: Hu et al. (2021) on LoRA efficiency, Taori et al. (2023) on synthetic data from large LLMs, and Zheng et al. (2023) on LLM-as-a-Judge. Our contribution is extending these findings to behavioral tasks and adding insight about the importance of data diversity.

\subsection{Limitations and Future Directions}

The research has some limitations to acknowledge: the 780-conversation dataset may not cover rare edge cases, experiments were only in English, and human evaluation with 30 test cases needs larger scaling. Regarding future directions, we are implementing GRPO to learn from human feedback, multi-task fine-tuning to other consulting domains, edge deployment with 4-bit quantization for mobile, and Vietnamese versions leveraging Qwen's multilingual capability.

\subsection{Conclusion}

This chapter has validated hypothesis H2: fine-tuning with PEFT is an effective method to adapt model behavior for specialized tasks. Combined with Advanced RAG in Chapter 3, these two methods together address both Knowledge Gap and Behavior Gap --- the two core challenges when deploying LLMs in enterprise environments.



%%------------------------------- BIBLIOGRAPHY ------------------------------------

\renewcommand{\bibname}{References}
\printbibliography[nottype=online, title={Bibliography}]
\printbibliography[type=online, heading=webography]
\addcontentsline{toc}{chapter}{\refname}

%%------------------------------- the end. ------------------------------------

\end{document}
